# C02_GPU_Programming

**æ‰€å±å­é¢†åŸŸ**: [B02_Graphics_3D](../README.md)
**åˆ›å»ºæ—¥æœŸ**: 2026-01-30
**æœ€åæ›´æ–°**: 2026-01-30

## ğŸ“‹ ä¸»é¢˜å®šä½

GPUç¼–ç¨‹ï¼ˆGPU Programmingï¼‰æ˜¯åˆ©ç”¨å›¾å½¢å¤„ç†å™¨å¹¶è¡Œè®¡ç®—èƒ½åŠ›è§£å†³é€šç”¨è®¡ç®—å’Œå›¾å½¢æ¸²æŸ“é—®é¢˜çš„æŠ€æœ¯é¢†åŸŸã€‚éšç€GPUè®¡ç®—èƒ½åŠ›çš„æŒ‡æ•°çº§å¢é•¿ï¼Œç°ä»£GPUç¼–ç¨‹å·²è¶…è¶Šä¼ ç»Ÿå›¾å½¢æ¸²æŸ“ï¼Œæ¶µç›–æ·±åº¦å­¦ä¹ ã€ç§‘å­¦è®¡ç®—ã€å¯†ç å­¦ã€æ•°æ®åˆ†æç­‰å¹¿æ³›é¢†åŸŸã€‚æŒæ¡GPUç¼–ç¨‹æ˜¯é‡Šæ”¾ç°ä»£ç¡¬ä»¶æ½œåŠ›çš„å…³é”®æŠ€èƒ½ã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### åŸºæœ¬å®šä¹‰

GPUç¼–ç¨‹é€šè¿‡åˆ©ç”¨GPUçš„å¤§è§„æ¨¡å¹¶è¡Œæ¶æ„ï¼ˆæ•°åƒä¸ªè½»é‡çº§æ ¸å¿ƒï¼‰åŠ é€Ÿè®¡ç®—å¯†é›†å‹ä»»åŠ¡ã€‚æ ¸å¿ƒèŒƒå¼åŒ…æ‹¬ï¼š
- **æ•°æ®å¹¶è¡Œ**ï¼šåŒä¸€æ“ä½œåº”ç”¨äºå¤§é‡æ•°æ®å…ƒç´ 
- **SIMTæ‰§è¡Œ**ï¼šå•æŒ‡ä»¤å¤šçº¿ç¨‹æ‰§è¡Œæ¨¡å‹
- **å†…å­˜å±‚æ¬¡**ï¼šå¯„å­˜å™¨ã€å…±äº«å†…å­˜ã€å…¨å±€å†…å­˜çš„ä¼˜åŒ–ä½¿ç”¨
- **è®¡ç®—-é€šä¿¡æ¯”**ï¼šæœ€å¤§åŒ–è®¡ç®—ã€æœ€å°åŒ–æ•°æ®ä¼ è¾“

### å…³é”®ç‰¹æ€§

**1. GPUæ¶æ„æ¼”è¿›**
- **NVIDIA CUDAæ ¸å¿ƒ**ï¼šSIMTæ‰§è¡Œã€Warpè°ƒåº¦ï¼ˆ32çº¿ç¨‹ï¼‰
- **AMD RDNA/CDNA**ï¼šWavefrontï¼ˆ64çº¿ç¨‹ï¼‰æ‰§è¡Œæ¨¡å‹
- **Intel Xe**ï¼šEUæ‰§è¡Œå•å…ƒã€çº¿ç¨‹çº§å¹¶è¡Œ
- **Apple Silicon GPU**ï¼šTBDRï¼ˆåˆ†å—å»¶è¿Ÿæ¸²æŸ“ï¼‰æ¶æ„

**2. ç¼–ç¨‹æ¨¡å‹**
- **CUDA**ï¼šNVIDIAä¸“æœ‰ï¼Œç”Ÿæ€æœ€æˆç†Ÿ
- **OpenCL**ï¼šè·¨å¹³å°å¼€æ”¾æ ‡å‡†
- **Vulkan/SYCL**ï¼šç°ä»£C++å¼‚æ„è®¡ç®—
- **HIP/ROCm**ï¼šAMDå¼€æºCUDAå…¼å®¹å±‚
- **WebGPU Compute**ï¼šæµè§ˆå™¨ç«¯GPUè®¡ç®—

**3. å†…å­˜æ¨¡å‹**
- **å…¨å±€å†…å­˜**ï¼šå®¹é‡å¤§ã€å»¶è¿Ÿé«˜ã€éœ€åˆå¹¶è®¿é—®
- **å…±äº«å†…å­˜/LDS**ï¼šç‰‡ä¸Šé«˜é€Ÿã€çº¿ç¨‹å—å…±äº«
- **å¯„å­˜å™¨**ï¼šæœ€å¿«é€Ÿã€æ•°é‡æœ‰é™
- **å¸¸é‡/çº¹ç†å†…å­˜**ï¼šç¼“å­˜ä¼˜åŒ–ã€åªè¯»è®¿é—®
- **ç»Ÿä¸€å†…å­˜**ï¼šè‡ªåŠ¨é¡µè¿ç§»ã€ç®€åŒ–ç¼–ç¨‹

**4. æ‰§è¡Œæ¨¡å‹**
- **Grid/Block/Thread**ï¼šä¸‰çº§å±‚æ¬¡ç»“æ„
- **Warp/Wavefront**ï¼šç¡¬ä»¶è°ƒåº¦å•ä½
- **å ç”¨ç‡**ï¼šæ´»è·ƒWarpæ¯”ä¾‹ï¼Œå½±å“å»¶è¿Ÿéšè—
- **å†…å­˜åˆå¹¶**ï¼šè¿ç»­çº¿ç¨‹è®¿é—®è¿ç»­å†…å­˜

### åº”ç”¨åœºæ™¯
- **æ·±åº¦å­¦ä¹ è®­ç»ƒæ¨ç†**ï¼šPyTorch/TensorFlowåº•å±‚åŠ é€Ÿ
- **ç§‘å­¦è®¡ç®—**ï¼šæµä½“åŠ¨åŠ›å­¦ã€åˆ†å­åŠ¨åŠ›å­¦ã€æ°”è±¡æ¨¡æ‹Ÿ
- **å›¾åƒå¤„ç†**ï¼šå®æ—¶æ»¤é•œã€è®¡ç®—æœºè§†è§‰ã€è§†é¢‘ç¼–ç 
- **å¯†ç å­¦**ï¼šåŒºå—é“¾æŒ–çŸ¿ã€åŠ å¯†è§£å¯†
- **é‡‘èå»ºæ¨¡**ï¼šè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿã€é£é™©åˆ†æ
- **å›¾å½¢æ¸²æŸ“**ï¼šå®æ—¶å…‰è¿½ã€å…¨å±€å…‰ç…§

## ğŸ› ï¸ æŠ€æœ¯å®è·µ

### å®ç°æ–¹æ³•

**1. CUDAåŸºç¡€ç¼–ç¨‹**

```cuda
// vector_add.cu - CUDAå‘é‡åŠ æ³•
#include <cuda_runtime.h>
#include <stdio.h>

// CUDAé”™è¯¯æ£€æŸ¥å®
#define CUDA_CHECK(call)                                                       \
    do {                                                                       \
        cudaError_t err = call;                                                \
        if (err != cudaSuccess) {                                              \
            fprintf(stderr, "CUDA error at %s:%d: %s\n", __FILE__, __LINE__,   \
                    cudaGetErrorString(err));                                  \
            exit(EXIT_FAILURE);                                                \
        }                                                                      \
    } while(0)

// å‘é‡åŠ æ³•æ ¸å‡½æ•°
__global__ void vectorAdd(const float* A, const float* B, float* C, int numElements) {
    // å…¨å±€çº¿ç¨‹IDè®¡ç®—
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    
    // è¾¹ç•Œæ£€æŸ¥
    if (i < numElements) {
        C[i] = A[i] + B[i];
    }
}

// ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨å‘é‡åŒ–åŠ è½½
__global__ void vectorAddVectorized(const float4* A, const float4* B, float4* C, int numElements) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < numElements / 4) {
        float4 a = A[i];
        float4 b = B[i];
        float4 c;
        c.x = a.x + b.x;
        c.y = a.y + b.y;
        c.z = a.z + b.z;
        c.w = a.w + b.w;
        C[i] = c;
    }
}

int main(void) {
    // å‘é‡å¤§å°
    int numElements = 50000;
    size_t size = numElements * sizeof(float);
    
    // ä¸»æœºå†…å­˜åˆ†é…
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);
    float* h_C = (float*)malloc(size);
    
    // åˆå§‹åŒ–æ•°æ®
    for (int i = 0; i < numElements; ++i) {
        h_A[i] = rand() / (float)RAND_MAX;
        h_B[i] = rand() / (float)RAND_MAX;
    }
    
    // è®¾å¤‡å†…å­˜åˆ†é…
    float* d_A = NULL;
    float* d_B = NULL;
    float* d_C = NULL;
    CUDA_CHECK(cudaMalloc((void**)&d_A, size));
    CUDA_CHECK(cudaMalloc((void**)&d_B, size));
    CUDA_CHECK(cudaMalloc((void**)&d_C, size));
    
    // æ•°æ®æ‹·è´åˆ°è®¾å¤‡
    CUDA_CHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice));
    
    // å¯åŠ¨æ ¸å‡½æ•°
    int threadsPerBlock = 256;
    int blocksPerGrid = (numElements + threadsPerBlock - 1) / threadsPerBlock;
    
    // åˆ›å»ºCUDAäº‹ä»¶ç”¨äºè®¡æ—¶
    cudaEvent_t start, stop;
    CUDA_CHECK(cudaEventCreate(&start));
    CUDA_CHECK(cudaEventCreate(&stop));
    
    CUDA_CHECK(cudaEventRecord(start));
    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements);
    CUDA_CHECK(cudaEventRecord(stop));
    
    CUDA_CHECK(cudaEventSynchronize(stop));
    float milliseconds = 0;
    CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));
    
    printf("Vector addition completed in %.3f ms\n", milliseconds);
    
    // ç»“æœæ‹·è´å›ä¸»æœº
    CUDA_CHECK(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost));
    
    // éªŒè¯ç»“æœ
    for (int i = 0; i < numElements; ++i) {
        if (fabs(h_A[i] + h_B[i] - h_C[i]) > 1e-5) {
            fprintf(stderr, "Result verification failed at element %d!\n", i);
            exit(EXIT_FAILURE);
        }
    }
    
    printf("Test PASSED\n");
    
    // é‡Šæ”¾èµ„æº
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));
    free(h_A);
    free(h_B);
    free(h_C);
    CUDA_CHECK(cudaEventDestroy(start));
    CUDA_CHECK(cudaEventDestroy(stop));
    
    return 0;
}
```

**2. å…±äº«å†…å­˜ä¼˜åŒ–ï¼šçŸ©é˜µä¹˜æ³•**

```cuda
// matrix_multiply.cu - ä¼˜åŒ–çŸ©é˜µä¹˜æ³•
#include <cuda_runtime.h>

#define TILE_WIDTH 16

// åŸºç¡€çŸ©é˜µä¹˜æ³•ï¼ˆå…¨å±€å†…å­˜ï¼‰
__global__ void matrixMulGlobal(float* C, float* A, float* B, int width) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < width && col < width) {
        float sum = 0.0f;
        for (int k = 0; k < width; k++) {
            sum += A[row * width + k] * B[k * width + col];
        }
        C[row * width + col] = sum;
    }
}

// å…±äº«å†…å­˜ä¼˜åŒ–çŸ©é˜µä¹˜æ³•
__global__ void matrixMulShared(float* C, float* A, float* B, int width) {
    __shared__ float ds_A[TILE_WIDTH][TILE_WIDTH];
    __shared__ float ds_B[TILE_WIDTH][TILE_WIDTH];
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * TILE_WIDTH + ty;
    int col = bx * TILE_WIDTH + tx;
    
    float Cvalue = 0.0f;
    
    // åˆ†å—éå†
    for (int m = 0; m < (width - 1) / TILE_WIDTH + 1; ++m) {
        // åä½œåŠ è½½Açš„å­çŸ©é˜µåˆ°å…±äº«å†…å­˜
        if (row < width && m * TILE_WIDTH + tx < width) {
            ds_A[ty][tx] = A[row * width + m * TILE_WIDTH + tx];
        } else {
            ds_A[ty][tx] = 0.0f;
        }
        
        // åä½œåŠ è½½Bçš„å­çŸ©é˜µåˆ°å…±äº«å†…å­˜
        if (m * TILE_WIDTH + ty < width && col < width) {
            ds_B[ty][tx] = B[(m * TILE_WIDTH + ty) * width + col];
        } else {
            ds_B[ty][tx] = 0.0f;
        }
        
        __syncthreads();
        
        // è®¡ç®—éƒ¨åˆ†ç§¯
        for (int k = 0; k < TILE_WIDTH; ++k) {
            Cvalue += ds_A[ty][k] * ds_B[k][tx];
        }
        
        __syncthreads();
    }
    
    if (row < width && col < width) {
        C[row * width + col] = Cvalue;
    }
}

// CUDA Coreä¼˜åŒ–ï¼šä½¿ç”¨å¯„å­˜å™¨åˆ†å—
__global__ void matrixMulOptimized(float* C, const float* A, const float* B, int M, int N, int K) {
    const int BM = 128;  // Block M dimension
    const int BN = 128;  // Block N dimension
    const int BK = 8;    // Block K dimension
    const int TM = 8;    // Thread M tiles
    const int TN = 8;    // Thread N tiles
    
    __shared__ float As[BM * BK];
    __shared__ float Bs[BK * BN];
    
    float regA[TM];
    float regB[TN];
    float threadResults[TM * TN] = {0.0f};
    
    int threadRow = threadIdx.x / (BN / TN);
    int threadCol = threadIdx.x % (BN / TN);
    
    // å¤–å±‚å¾ªç¯éå†Kç»´åº¦
    for (int bkIdx = 0; bkIdx < K; bkIdx += BK) {
        // åŠ è½½Aåˆ°å…±äº«å†…å­˜
        for (int i = 0; i < BM * BK / blockDim.x; i++) {
            int row = blockIdx.y * BM + (i * blockDim.x + threadIdx.x) / BK;
            int col = bkIdx + (i * blockDim.x + threadIdx.x) % BK;
            if (row < M && col < K) {
                As[(i * blockDim.x + threadIdx.x)] = A[row * K + col];
            }
        }
        
        // åŠ è½½Båˆ°å…±äº«å†…å­˜
        for (int i = 0; i < BK * BN / blockDim.x; i++) {
            int row = bkIdx + (i * blockDim.x + threadIdx.x) / BN;
            int col = blockIdx.x * BN + (i * blockDim.x + threadIdx.x) % BN;
            if (row < K && col < N) {
                Bs[(i * blockDim.x + threadIdx.x)] = B[row * N + col];
            }
        }
        
        __syncthreads();
        
        // å†…å±‚å¾ªç¯è®¡ç®—
        for (int dotIdx = 0; dotIdx < BK; ++dotIdx) {
            // åŠ è½½Aå¯„å­˜å™¨
            for (int i = 0; i < TM; ++i) {
                regA[i] = As[(threadRow * TM + i) * BK + dotIdx];
            }
            // åŠ è½½Bå¯„å­˜å™¨
            for (int i = 0; i < TN; ++i) {
                regB[i] = Bs[dotIdx * BN + threadCol * TN + i];
            }
            // è®¡ç®—å¤–ç§¯
            for (int rm = 0; rm < TM; ++rm) {
                for (int rn = 0; rn < TN; ++rn) {
                    threadResults[rm * TN + rn] += regA[rm] * regB[rn];
                }
            }
        }
        
        __syncthreads();
    }
    
    // å†™å›ç»“æœ
    for (int rm = 0; rm < TM; ++rm) {
        for (int rn = 0; rn < TN; ++rn) {
            int row = blockIdx.y * BM + threadRow * TM + rm;
            int col = blockIdx.x * BN + threadCol * TN + rn;
            if (row < M && col < N) {
                C[row * N + col] = threadResults[rm * TN + rn];
            }
        }
    }
}

// cuBLASè°ƒç”¨ç¤ºä¾‹
void matrixMulCuBLAS(float* C, const float* A, const float* B, int m, int n, int k) {
    cublasHandle_t handle;
    cublasCreate(&handle);
    
    const float alpha = 1.0f;
    const float beta = 0.0f;
    
    // C = alpha * A * B + beta * C
    // æ³¨æ„cuBLASæ˜¯åˆ—ä¼˜å…ˆå­˜å‚¨
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
                n, m, k,
                &alpha,
                B, n,
                A, k,
                &beta,
                C, n);
    
    cublasDestroy(handle);
}
```

**3. å½’çº¦ç®—æ³•ä¸å¹¶è¡Œå‰ç¼€å’Œ**

```cuda
// reduction.cu - å¹¶è¡Œå½’çº¦ç®—æ³•
#include <cuda_runtime.h>

// åŸºç¡€å½’çº¦ï¼šçº¿ç¨‹çº§å½’çº¦
__global__ void reduceSumBasic(float* input, float* output, int n) {
    extern __shared__ float sdata[];
    
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x * blockDim.x * 2 + threadIdx.x;
    
    // åŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸¤ä¸ªå…ƒç´ 
    sdata[tid] = (i < n) ? input[i] : 0;
    if (i + blockDim.x < n) {
        sdata[tid] += input[i + blockDim.x];
    }
    __syncthreads();
    
    // æ ‘å½¢å½’çº¦
    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    
    // å†™å›ç»“æœ
    if (tid == 0) {
        output[blockIdx.x] = sdata[0];
    }
}

// ä¼˜åŒ–å½’çº¦ï¼šå±•å¼€å¾ªç¯
__global__ void reduceSumUnrolled(float* input, float* output, int n) {
    extern __shared__ float sdata[];
    
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x * (blockDim.x * 2) + threadIdx.x;
    
    float sum = 0.0f;
    // å±•å¼€4xï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†8ä¸ªå…ƒç´ 
    #pragma unroll
    for (int j = 0; j < 4; j++) {
        unsigned int idx = i + j * blockDim.x * 2;
        if (idx < n) sum += input[idx];
        if (idx + blockDim.x < n) sum += input[idx + blockDim.x];
    }
    sdata[tid] = sum;
    __syncthreads();
    
    // å±•å¼€å½’çº¦å¾ªç¯
    if (blockDim.x >= 512 && tid < 256) sdata[tid] += sdata[tid + 256]; __syncthreads();
    if (blockDim.x >= 256 && tid < 128) sdata[tid] += sdata[tid + 128]; __syncthreads();
    if (blockDim.x >= 128 && tid < 64) sdata[tid] += sdata[tid + 64]; __syncthreads();
    
    // æœ€åwarpä½¿ç”¨shuffleæŒ‡ä»¤
    if (tid < 32) {
        volatile float* vsmem = sdata;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid + 8];
        vsmem[tid] += vsmem[tid + 4];
        vsmem[tid] += vsmem[tid + 2];
        vsmem[tid] += vsmem[tid + 1];
    }
    
    if (tid == 0) {
        output[blockIdx.x] = sdata[0];
    }
}

// Warpçº§å½’çº¦ä½¿ç”¨shuffleæŒ‡ä»¤
__inline__ __device__ float warpReduceSum(float val) {
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xFFFFFFFF, val, offset);
    }
    return val;
}

// å¹¶è¡Œå‰ç¼€å’Œï¼ˆScanï¼‰
__global__ void scanBlock(float* input, float* output, float* blockSums, int n) {
    extern __shared__ float temp[];
    
    int tid = threadIdx.x;
    int offset = 1;
    int ai = tid;
    int bi = tid + (n / 2);
    
    // åŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜
    float t = input[ai];
    temp[ai] = t;
    t = input[bi];
    temp[bi] = t;
    
    // ä¸Šæ‰«é˜¶æ®µ
    for (int d = n >> 1; d > 0; d >>= 1) {
        __syncthreads();
        if (tid < d) {
            int ai = offset * (2 * tid + 1) - 1;
            int bi = offset * (2 * tid + 2) - 1;
            temp[bi] += temp[ai];
        }
        offset *= 2;
    }
    
    // æ¸…é›¶æœ€åä¸€ä¸ªå…ƒç´ 
    if (tid == 0) {
        if (blockSums != NULL) {
            blockSums[blockIdx.x] = temp[n - 1];
        }
        temp[n - 1] = 0;
    }
    
    // ä¸‹æ‰«é˜¶æ®µ
    for (int d = 1; d < n; d *= 2) {
        offset >>= 1;
        __syncthreads();
        if (tid < d) {
            int ai = offset * (2 * tid + 1) - 1;
            int bi = offset * (2 * tid + 2) - 1;
            float t = temp[ai];
            temp[ai] = temp[bi];
            temp[bi] += t;
        }
    }
    __syncthreads();
    
    // å†™å›ç»“æœ
    output[ai] = temp[ai];
    output[bi] = temp[bi];
}

// Thruståº“è°ƒç”¨
#include <thrust/device_vector.h>
#include <thrust/reduce.h>
#include <thrust/scan.h>

void reduceWithThrust(float* d_input, float* d_output, int n) {
    thrust::device_ptr<float> dev_ptr(d_input);
    float sum = thrust::reduce(dev_ptr, dev_ptr + n);
    cudaMemcpy(d_output, &sum, sizeof(float), cudaMemcpyHostToDevice);
}

void scanWithThrust(float* d_input, float* d_output, int n) {
    thrust::device_ptr<float> in_ptr(d_input);
    thrust::device_ptr<float> out_ptr(d_output);
    thrust::exclusive_scan(in_ptr, in_ptr + n, out_ptr);
}
```

**4. OpenCLè·¨å¹³å°ç¼–ç¨‹**

```cpp
// opencl_vector_add.cpp
#define CL_TARGET_OPENCL_VERSION 300
#include <CL/cl.h>
#include <vector>
#include <iostream>
#include <fstream>
#include <sstream>

const char* kernelSource = R"(
__kernel void vectorAdd(
    __global const float* A,
    __global const float* B,
    __global float* C,
    const int numElements)
{
    int i = get_global_id(0);
    if (i < numElements) {
        C[i] = A[i] + B[i];
    }
}
)";

class OpenCLContext {
public:
    bool initialize() {
        cl_int err;
        
        // è·å–å¹³å°
        cl_uint numPlatforms;
        err = clGetPlatformIDs(0, NULL, &numPlatforms);
        if (err != CL_SUCCESS || numPlatforms == 0) {
            std::cerr << "Failed to find OpenCL platform\n";
            return false;
        }
        
        std::vector<cl_platform_id> platforms(numPlatforms);
        clGetPlatformIDs(numPlatforms, platforms.data(), NULL);
        
        // é€‰æ‹©ç¬¬ä¸€ä¸ªå¹³å°
        platform = platforms[0];
        
        // è·å–è®¾å¤‡
        cl_uint numDevices;
        err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 0, NULL, &numDevices);
        if (err != CL_SUCCESS || numDevices == 0) {
            // å›é€€åˆ°CPU
            err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_CPU, 0, NULL, &numDevices);
            if (err != CL_SUCCESS || numDevices == 0) {
                std::cerr << "Failed to find OpenCL device\n";
                return false;
            }
        }
        
        std::vector<cl_device_id> devices(numDevices);
        clGetDeviceIDs(platform, CL_DEVICE_TYPE_ALL, numDevices, devices.data(), NULL);
        device = devices[0];
        
        // åˆ›å»ºè®¾å¤‡ä¸Šä¸‹æ–‡
        context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to create context\n";
            return false;
        }
        
        // åˆ›å»ºå‘½ä»¤é˜Ÿåˆ—
        commandQueue = clCreateCommandQueueWithProperties(context, device, 0, &err);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to create command queue\n";
            return false;
        }
        
        // åˆ›å»ºç¨‹åº
        program = clCreateProgramWithSource(context, 1, &kernelSource, NULL, &err);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to create program\n";
            return false;
        }
        
        // ç¼–è¯‘ç¨‹åº
        err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);
        if (err != CL_SUCCESS) {
            size_t logSize;
            clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, 0, NULL, &logSize);
            std::vector<char> log(logSize);
            clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, logSize, log.data(), NULL);
            std::cerr << "Build error:\n" << log.data() << "\n";
            return false;
        }
        
        // åˆ›å»ºæ ¸å‡½æ•°
        kernel = clCreateKernel(program, "vectorAdd", &err);
        if (err != CL_SUCCESS) {
            std::cerr << "Failed to create kernel\n";
            return false;
        }
        
        return true;
    }
    
    void vectorAdd(const std::vector<float>& a, const std::vector<float>& b, 
                   std::vector<float>& c, int numElements) {
        cl_int err;
        size_t size = numElements * sizeof(float);
        
        // åˆ›å»ºè®¾å¤‡å†…å­˜
        cl_mem d_a = clCreateBuffer(context, CL_MEM_READ_ONLY, size, NULL, &err);
        cl_mem d_b = clCreateBuffer(context, CL_MEM_READ_ONLY, size, NULL, &err);
        cl_mem d_c = clCreateBuffer(context, CL_MEM_WRITE_ONLY, size, NULL, &err);
        
        // æ‹·è´æ•°æ®åˆ°è®¾å¤‡
        clEnqueueWriteBuffer(commandQueue, d_a, CL_TRUE, 0, size, a.data(), 0, NULL, NULL);
        clEnqueueWriteBuffer(commandQueue, d_b, CL_TRUE, 0, size, b.data(), 0, NULL, NULL);
        
        // è®¾ç½®æ ¸å‡½æ•°å‚æ•°
        clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_a);
        clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_b);
        clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_c);
        clSetKernelArg(kernel, 3, sizeof(int), &numElements);
        
        // æ‰§è¡Œæ ¸å‡½æ•°
        size_t globalSize = ((numElements + 255) / 256) * 256;
        size_t localSize = 256;
        clEnqueueNDRangeKernel(commandQueue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);
        
        // è¯»å–ç»“æœ
        clEnqueueReadBuffer(commandQueue, d_c, CL_TRUE, 0, size, c.data(), 0, NULL, NULL);
        
        // é‡Šæ”¾èµ„æº
        clReleaseMemObject(d_a);
        clReleaseMemObject(d_b);
        clReleaseMemObject(d_c);
    }
    
    ~OpenCLContext() {
        clReleaseKernel(kernel);
        clReleaseProgram(program);
        clReleaseCommandQueue(commandQueue);
        clReleaseContext(context);
    }
    
private:
    cl_platform_id platform;
    cl_device_id device;
    cl_context context;
    cl_command_queue commandQueue;
    cl_program program;
    cl_kernel kernel;
};
```

**5. HIP/ROCm AMD GPUç¼–ç¨‹**

```cpp
// hip_matrix_multiply.cpp
#include <hip/hip_runtime.h>
#include <iostream>

// HIPå…¼å®¹CUDAä»£ç ï¼Œåªéœ€æ›¿æ¢å¤´æ–‡ä»¶å’Œå…³é”®å­—
// __global__ -> __global__
// __shared__ -> __shared__
// __syncthreads() -> __syncthreads()
// threadIdx -> hipThreadIdx_
// blockIdx -> hipBlockIdx_
// blockDim -> hipBlockDim_

#define HIP_CHECK(call)                                                         \
    do {                                                                        \
        hipError_t err = call;                                                  \
        if (err != hipSuccess) {                                                \
            std::cerr << "HIP error at " << __FILE__ << ":" << __LINE__;        \
            std::cerr << " - " << hipGetErrorString(err) << std::endl;          \
            exit(EXIT_FAILURE);                                                 \
        }                                                                       \
    } while(0)

// çŸ©é˜µä¹˜HIPæ ¸å‡½æ•°
__global__ void matrixMulHIP(float* C, const float* A, const float* B, 
                             int M, int N, int K) {
    int row = hipBlockIdx_y * hipBlockDim_y + hipThreadIdx_y;
    int col = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;
    
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += A[row * K + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

// ä½¿ç”¨rocBLAS
#include <rocblas.h>

void matrixMulRocBLAS(float* d_C, float* d_A, float* d_B, int m, int n, int k) {
    rocblas_handle handle;
    rocblas_create_handle(&handle);
    
    const float alpha = 1.0f;
    const float beta = 0.0f;
    
    // C = alpha * A * B + beta * C
    rocblas_sgemm(handle, rocblas_operation_none, rocblas_operation_none,
                  n, m, k,
                  &alpha,
                  d_B, n,
                  d_A, k,
                  &beta,
                  d_C, n);
    
    rocblas_destroy_handle(handle);
}

// ä½¿ç”¨rocThrust
#include <thrust/device_vector.h>
#include <thrust/reduce.h>

void reduceRocThrust(float* d_input, int n) {
    thrust::device_ptr<float> dev_ptr(d_input);
    float sum = thrust::reduce(dev_ptr, dev_ptr + n);
    std::cout << "Sum: " << sum << std::endl;
}
```

**6. SYCLç°ä»£C++å¼‚æ„ç¼–ç¨‹**

```cpp
// sycl_vector_add.cpp
#include <sycl/sycl.hpp>
#include <vector>
#include <iostream>

void vectorAddSYCL(const std::vector<float>& a, const std::vector<float>& b,
                   std::vector<float>& c, int numElements) {
    // åˆ›å»ºSYCLé˜Ÿåˆ—
    sycl::queue q(sycl::default_selector_v);
    
    std::cout << "Running on: " 
              << q.get_device().get_info<sycl::info::device::name>() << std::endl;
    
    // åˆ›å»ºç¼“å†²åŒº
    sycl::buffer<float, 1> buf_a(a.data(), sycl::range<1>(numElements));
    sycl::buffer<float, 1> buf_b(b.data(), sycl::range<1>(numElements));
    sycl::buffer<float, 1> buf_c(c.data(), sycl::range<1>(numElements));
    
    // æäº¤å‘½ä»¤ç»„
    q.submit([&](sycl::handler& h) {
        // åˆ›å»ºè®¿é—®å™¨
        auto acc_a = buf_a.get_access<sycl::access::mode::read>(h);
        auto acc_b = buf_b.get_access<sycl::access::mode::read>(h);
        auto acc_c = buf_c.get_access<sycl::access::mode::write>(h);
        
        // å¹¶è¡Œfor
        h.parallel_for(sycl::range<1>(numElements), [=](sycl::id<1> i) {
            acc_c[i] = acc_a[i] + acc_b[i];
        });
    });
    
    // ç¼“å†²åŒºè‡ªåŠ¨åŒæ­¥
}

// SYCLçŸ©é˜µä¹˜æ³•
void matrixMulSYCL(const std::vector<float>& A, const std::vector<float>& B,
                   std::vector<float>& C, int M, int N, int K) {
    sycl::queue q(sycl::gpu_selector_v);
    
    const int TILE_SIZE = 16;
    
    sycl::buffer<float, 2> buf_A(A.data(), sycl::range<2>(M, K));
    sycl::buffer<float, 2> buf_B(B.data(), sycl::range<2>(K, N));
    sycl::buffer<float, 2> buf_C(C.data(), sycl::range<2>(M, N));
    
    q.submit([&](sycl::handler& h) {
        // æœ¬åœ°å†…å­˜è®¿é—®å™¨
        sycl::local_accessor<float, 2> tile_A(sycl::range<2>(TILE_SIZE, TILE_SIZE), h);
        sycl::local_accessor<float, 2> tile_B(sycl::range<2>(TILE_SIZE, TILE_SIZE), h);
        
        auto acc_A = buf_A.get_access<sycl::access::mode::read>(h);
        auto acc_B = buf_B.get_access<sycl::access::mode::read>(h);
        auto acc_C = buf_C.get_access<sycl::access::mode::write>(h);
        
        // ND-rangeå¹¶è¡Œ
        h.parallel_for(
            sycl::nd_range<2>(
                sycl::range<2>((M + TILE_SIZE - 1) / TILE_SIZE * TILE_SIZE,
                              (N + TILE_SIZE - 1) / TILE_SIZE * TILE_SIZE),
                sycl::range<2>(TILE_SIZE, TILE_SIZE)
            ),
            [=](sycl::nd_item<2> item) {
                int row = item.get_global_id(0);
                int col = item.get_global_id(1);
                int local_row = item.get_local_id(0);
                int local_col = item.get_local_id(1);
                
                float sum = 0.0f;
                
                // åˆ†å—å¾ªç¯
                for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {
                    // åä½œåŠ è½½åˆ°æœ¬åœ°å†…å­˜
                    if (row < M && t * TILE_SIZE + local_col < K) {
                        tile_A[local_row][local_col] = acc_A[row][t * TILE_SIZE + local_col];
                    } else {
                        tile_A[local_row][local_col] = 0.0f;
                    }
                    
                    if (t * TILE_SIZE + local_row < K && col < N) {
                        tile_B[local_row][local_col] = acc_B[t * TILE_SIZE + local_row][col];
                    } else {
                        tile_B[local_row][local_col] = 0.0f;
                    }
                    
                    item.barrier(sycl::access::fence_space::local_space);
                    
                    // è®¡ç®—éƒ¨åˆ†ç§¯
                    for (int k = 0; k < TILE_SIZE; k++) {
                        sum += tile_A[local_row][k] * tile_B[k][local_col];
                    }
                    
                    item.barrier(sycl::access::fence_space::local_space);
                }
                
                if (row < M && col < N) {
                    acc_C[row][col] = sum;
                }
            }
        );
    });
}

// oneMKL BLASè°ƒç”¨
#include <oneapi/mkl/blas.hpp>

void matrixMulOneMKL(sycl::queue& q, float* A, float* B, float* C, int m, int n, int k) {
    float alpha = 1.0f;
    float beta = 0.0f;
    
    oneapi::mkl::blas::row_major::gemm(
        q,
        oneapi::mkl::transpose::nontrans,
        oneapi::mkl::transpose::nontrans,
        m, n, k,
        alpha,
        A, k,
        B, n,
        beta,
        C, n
    );
}
```

### æœ€ä½³å®è·µ

**1. å†…å­˜è®¿é—®ä¼˜åŒ–**
- å…¨å±€å†…å­˜åˆå¹¶è®¿é—®
- å…±äº«å†…å­˜é¿å…bank conflict
- çº¹ç†å†…å­˜ç”¨äºéšæœºè®¿é—®
- å¸¸é‡å†…å­˜ç”¨äºå¹¿æ’­æ•°æ®

**2. å ç”¨ç‡ä¼˜åŒ–**
```cpp
// è®¡ç®—å ç”¨ç‡
cudaOccMaxActiveBlocksPerMultiprocessor(
    &numBlocks,
    prop,
    kernel,
    blockSize,
    dynamicSmemSize
);
```

**3. æµä¸å¹¶å‘**
```cpp
// å¤šæµå¹¶å‘æ‰§è¡Œ
cudaStream_t streams[4];
for (int i = 0; i < 4; i++) {
    cudaStreamCreate(&streams[i]);
}

// å¼‚æ­¥æ•°æ®ä¼ è¾“ä¸è®¡ç®—
for (int i = 0; i < 4; i++) {
    cudaMemcpyAsync(d_data + offset, h_data + offset, size, 
                    cudaMemcpyHostToDevice, streams[i]);
    kernel<<<blocks, threads, 0, streams[i]>>>(d_data + offset);
    cudaMemcpyAsync(h_result + offset, d_result + offset, size,
                    cudaMemcpyDeviceToHost, streams[i]);
}

cudaDeviceSynchronize();
```

**4. ç»Ÿä¸€å†…å­˜**
```cpp
// ç»Ÿä¸€å†…å­˜è‡ªåŠ¨ç®¡ç†
cudaMallocManaged(&data, size);

// é¢„å–ä¼˜åŒ–
cudaMemPrefetchAsync(data, size, deviceId, stream);
```

### å¸¸è§é™·é˜±

**1. å†…å­˜æ³„æ¼**
- âŒ é—®é¢˜ï¼šcudaMallocåæœªcudaFree
- âœ… è§£å†³ï¼šä½¿ç”¨RAIIåŒ…è£…ç±»

**2. åŒæ­¥é—®é¢˜**
- âŒ é—®é¢˜ï¼šå¼‚æ­¥æ“ä½œå¯¼è‡´æ•°æ®ç«äº‰
- âœ… è§£å†³ï¼šæ­£ç¡®ä½¿ç”¨cudaDeviceSynchronize

**3. å¯„å­˜å™¨å‹åŠ›**
- âŒ é—®é¢˜ï¼šè¿‡å¤šå±€éƒ¨å˜é‡é™ä½å ç”¨ç‡
- âœ… è§£å†³ï¼šåˆ†æå¹¶å‡å°‘å¯„å­˜å™¨ä½¿ç”¨

**4. çº¿ç¨‹å‘æ•£**
- âŒ é—®é¢˜ï¼šWarpå†…çº¿ç¨‹èµ°ä¸åŒåˆ†æ”¯
- âœ… è§£å†³ï¼šé‡æ„ç®—æ³•å‡å°‘åˆ†æ”¯

## ğŸ“š èµ„æºç´¢å¼•

### å­¦æœ¯è®ºæ–‡

1. **CUDA Programming Guide** (NVIDIA)
   - https://docs.nvidia.com/cuda/cuda-c-programming-guide/
   - CUDAç¼–ç¨‹å®˜æ–¹æƒå¨æŒ‡å—

2. **Programming Massively Parallel Processors, 4th Edition** (2022)
   - ä½œè€…ï¼šDavid B. Kirk, Wen-mei W. Hwu
   - CUDAç¼–ç¨‹æ ‡å‡†æ•™æ

3. **OpenCL Specification** (Khronos)
   - https://www.khronos.org/opencl/
   - OpenCLå¼€æ”¾æ ‡å‡†è§„èŒƒ

4. **SYCL 2020 Specification** (Khronos)
   - https://www.khronos.org/sycl/
   - ç°ä»£C++å¼‚æ„ç¼–ç¨‹æ ‡å‡†

5. **HIP Programming Guide** (AMD)
   - https://rocm.docs.amd.com/
   - AMD GPUç¼–ç¨‹æŒ‡å—

### æŠ€æœ¯æ–‡æ¡£

1. **NVIDIA CUDA Best Practices Guide**
   - https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
   - æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

2. **NVIDIA Nsight Documentation**
   - æ€§èƒ½åˆ†æä¸è°ƒè¯•å·¥å…·

3. **Intel oneAPI DPC++ Documentation**
   - https://spec.oneapi.io/oneapi-spec.pdf
   - Intelå¼‚æ„ç¼–ç¨‹æ¡†æ¶

### å¼€æºé¡¹ç›®

1. **cuDNN** - https://developer.nvidia.com/cudnn
   - æ·±åº¦å­¦ä¹ åŸºå…ƒåº“

2. **cuBLAS/cuFFT/cuRAND** - CUDA Math Libraries
   - NVIDIAæ•°å­¦åº“

3. **Thrust** - https://github.com/NVIDIA/thrust
   - å¹¶è¡Œç®—æ³•åº“

4. **CUB** - https://github.com/NVIDIA/cub
   - CUDAå—çº§å¹¶è¡ŒåŸè¯­

5. **hipBLAS/hipFFT** - https://github.com/ROCmSoftwarePlatform
   - AMD ROCmæ•°å­¦åº“

6. **oneMKL** - https://github.com/oneapi-src/oneMKL
   - Intel oneAPIæ•°å­¦åº“

### å·¥å…·ä¸æ¡†æ¶

1. **Nsight Compute/Systems**
   - NVIDIAæ€§èƒ½åˆ†æå·¥å…·

2. **rocProf**
   - AMD GPUåˆ†æå™¨

3. **Intel VTune**
   - Intelæ€§èƒ½åˆ†æ

4. **GPU Burn**
   - GPUå‹åŠ›æµ‹è¯•å·¥å…·

## ğŸ”— å…³è”çŸ¥è¯†

```mermaid
graph TD
    C02[C02_GPU_Programming]

    C02 --> C01[C01_Real-Time_Rendering]
    C02 --> B01[B01_AI_LLM_Engineering]
    
    C02 -.-> A01[A06_Technical_Intuition/B04_Performance_Optimization]
    C02 -.-> A02[A06_Technical_Intuition/B06_System_Architecture]
    
    B01 --> |æ·±åº¦å­¦ä¹ åŠ é€Ÿ| C02
```

## ğŸ’¡ å­¦ä¹ å»ºè®®

### å‰ç½®çŸ¥è¯†
- C/C++ç¼–ç¨‹åŸºç¡€
- å¹¶è¡Œè®¡ç®—æ¦‚å¿µ
- è®¡ç®—æœºä½“ç³»ç»“æ„åŸºç¡€
- çº¿æ€§ä»£æ•°

### å­¦ä¹ è·¯å¾„

**ç¬¬1-2å‘¨ï¼šCUDAåŸºç¡€**
- å†…å­˜ç®¡ç†ã€æ ¸å‡½æ•°å¯åŠ¨
- å®è·µï¼šå‘é‡æ“ä½œã€çŸ©é˜µè½¬ç½®
- å·¥å…·ï¼šNsightè°ƒè¯•

**ç¬¬3-4å‘¨ï¼šæ€§èƒ½ä¼˜åŒ–**
- å…±äº«å†…å­˜ã€åˆå¹¶è®¿é—®
- å®è·µï¼šä¼˜åŒ–çŸ©é˜µä¹˜æ³•
- ç›®æ ‡ï¼šè¾¾åˆ°å³°å€¼æ€§èƒ½80%

**ç¬¬5-6å‘¨ï¼šé«˜çº§ä¸»é¢˜**
- åŠ¨æ€å¹¶è¡Œã€ç»Ÿä¸€å†…å­˜
- å¤šGPUç¼–ç¨‹
- å®è·µï¼šå¤§è§„æ¨¡å¹¶è¡Œåº”ç”¨

**ç¬¬7-8å‘¨ï¼šè·¨å¹³å°ç¼–ç¨‹**
- OpenCLæˆ–SYCL
- å¯ç§»æ¤ä»£ç è®¾è®¡
- å®è·µï¼šè·¨å¹³å°åº”ç”¨

### å®è·µé¡¹ç›®

**é¡¹ç›®1ï¼šå¹¶è¡Œæ’åºç®—æ³•**
- å®ç°ï¼šBitonicæ’åºã€å½’å¹¶æ’åº
- ç›®æ ‡ï¼šæ¯”CPUå¿«50x

**é¡¹ç›®2ï¼šå›¾åƒå·ç§¯åŠ é€Ÿ**
- å®ç°ï¼šé«˜æ–¯æ¨¡ç³Šã€è¾¹ç¼˜æ£€æµ‹
- ä¼˜åŒ–ï¼šå…±äº«å†…å­˜ã€å¸¸é‡å†…å­˜

**é¡¹ç›®3ï¼šè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ**
- å®ç°ï¼šæœŸæƒå®šä»·ã€é£é™©åˆ†æ
- æŠ€æœ¯ï¼šcuRANDã€å¹¶è¡Œè§„çº¦

**é¡¹ç›®4ï¼šåˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ**
- å®ç°ï¼šNä½“é—®é¢˜ã€åŠ›è®¡ç®—
- ä¼˜åŒ–ï¼šç©ºé—´åˆ’åˆ†ã€å…±äº«å†…å­˜

## ğŸ”„ ç»´æŠ¤è¯´æ˜

- **æ›´æ–°é¢‘ç‡**: æ¯å­£åº¦æ›´æ–°ï¼Œè·Ÿè¸ªæ–°GPUæ¶æ„
- **è´¨é‡æ ‡å‡†**: æ‰€æœ‰ä»£ç åœ¨å¤šå¹³å°æµ‹è¯•
- **è´¡çŒ®æ–¹å¼**: æäº¤æ–°ä¼˜åŒ–æŠ€æœ¯ã€æ¶æ„é€‚é…
