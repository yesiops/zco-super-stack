# C02 Model Fine-Tuning

**æ‰€å±å­é¢†åŸŸ**: [B01_AI_LLM_Engineering](../README.md)  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-30  
**æœ€åæ›´æ–°**: 2026-01-30

## ğŸ“‹ ä¸»é¢˜å®šä½

æ¨¡å‹å¾®è°ƒï¼ˆModel Fine-Tuningï¼‰æ˜¯å¤§è¯­è¨€æ¨¡å‹å·¥ç¨‹åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šç»§ç»­è®­ç»ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç›¸æ¯”æç¤ºå·¥ç¨‹ï¼Œå¾®è°ƒèƒ½å¤Ÿå®ç°æ›´æ·±åº¦çš„æ¨¡å‹é€‚é…ï¼Œæ˜¾è‘—æå‡ä¸“ä¸šé¢†åŸŸä»»åŠ¡çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### åŸºæœ¬å®šä¹‰

æ¨¡å‹å¾®è°ƒæ˜¯åœ¨é¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€LLaMAã€Claude ç­‰ï¼‰çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡çš„æ•°æ®é›†è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œä½¿æ¨¡å‹å­¦ä¹ é¢†åŸŸçŸ¥è¯†å’Œä»»åŠ¡æ¨¡å¼çš„è¿‡ç¨‹ã€‚

**å¾®è°ƒ vs é¢„è®­ç»ƒ vs æç¤ºå·¥ç¨‹**:

| ç»´åº¦ | é¢„è®­ç»ƒ | å¾®è°ƒ | æç¤ºå·¥ç¨‹ |
|------|--------|------|----------|
| **æ•°æ®é‡** | ä¸‡äº¿çº§ token | åƒ-ç™¾ä¸‡çº§æ ·æœ¬ | æ— éœ€è®­ç»ƒæ•°æ® |
| **è®¡ç®—æˆæœ¬** | æé«˜ï¼ˆç™¾ä¸‡ç¾å…ƒçº§ï¼‰ | ä¸­ç­‰ï¼ˆæ•°ç™¾-æ•°åƒç¾å…ƒï¼‰ | ä½ï¼ˆæ¨ç†æˆæœ¬ï¼‰ |
| **é¢†åŸŸé€‚åº”** | é€šç”¨èƒ½åŠ› | æ·±åº¦é¢†åŸŸé€‚é… | ä¾èµ–ä¸Šä¸‹æ–‡å­¦ä¹  |
| **çŸ¥è¯†æ›´æ–°** | åŸºç¡€ä¸–ç•ŒçŸ¥è¯† | é¢†åŸŸä¸“ä¸šçŸ¥è¯† | åŠ¨æ€ä¿¡æ¯æ³¨å…¥ |
| **é€‚ç”¨åœºæ™¯** | åŸºç¡€æ¨¡å‹æ„å»º | ä¸“ä¸šé¢†åŸŸåº”ç”¨ | å¿«é€ŸåŸå‹éªŒè¯ |

### å…³é”®ç‰¹æ€§

**1. å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰**
- æ›´æ–°æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
- æ•ˆæœæœ€ä½³ï¼Œä½†è®¡ç®—æˆæœ¬æœ€é«˜
- éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®å’Œ GPU èµ„æº
- å®¹æ˜“äº§ç”Ÿç¾éš¾æ€§é—å¿˜

**2. å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰**
- **LoRA (Low-Rank Adaptation)**: ä½ç§©é€‚é…ï¼Œåªè®­ç»ƒä½ç§©çŸ©é˜µ
- **QLoRA**: 4-bit é‡åŒ– + LoRAï¼Œå¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚
- **Adapter**: åœ¨ Transformer å±‚é—´æ’å…¥å°å‹é€‚é…å™¨æ¨¡å—
- **Prefix Tuning**: è®­ç»ƒå‰ç¼€åµŒå…¥ï¼Œå†»ç»“ä¸»ä½“å‚æ•°
- **Prompt Tuning**: ä¼˜åŒ–è½¯æç¤ºå‘é‡

**3. æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰**
- ä½¿ç”¨ (æŒ‡ä»¤, è¾“å…¥, è¾“å‡º) æ ¼å¼æ•°æ®è®­ç»ƒ
- ä½¿æ¨¡å‹å­¦ä¼šéµå¾ªäººç±»æŒ‡ä»¤
- æ˜¯æ„å»º Chat æ¨¡å‹çš„å…³é”®æ­¥éª¤
- å…¸å‹æ•°æ®é›†ï¼šAlpacaã€Dollyã€OpenAssistant

**4. äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**
- SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰â†’ RMï¼ˆå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼‰â†’ RLï¼ˆå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ï¼‰
- ä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½
- å…³é”®æ­¥éª¤ï¼šæ•°æ®æ”¶é›† â†’ å¥–åŠ±å»ºæ¨¡ â†’ PPO/DPO è®­ç»ƒ

### åº”ç”¨åœºæ™¯

- **å‚ç›´é¢†åŸŸæ¨¡å‹**: æ³•å¾‹ã€åŒ»ç–—ã€é‡‘èç­‰ä¸“ä¸šé¢†åŸŸåŠ©æ‰‹
- **ä»£ç ç”Ÿæˆ**: ç‰¹å®šç¼–ç¨‹è¯­è¨€æˆ–æ¡†æ¶çš„ä»£ç è¡¥å…¨
- **å¤šè¯­è¨€æ”¯æŒ**: ä½èµ„æºè¯­è¨€çš„æ¨¡å‹é€‚é…
- **é£æ ¼è¿ç§»**: ç‰¹å®šå†™ä½œé£æ ¼æˆ–è¯­æ°”çš„ç”Ÿæˆ
- **ä»»åŠ¡ç‰¹åŒ–**: æ‘˜è¦ã€ç¿»è¯‘ã€åˆ†ç±»ç­‰ç‰¹å®šä»»åŠ¡ä¼˜åŒ–

## ğŸ› ï¸ æŠ€æœ¯å®è·µ

### å®ç°æ–¹æ³•

**1. æ•°æ®å‡†å¤‡**

```python
# æ ‡å‡†æŒ‡ä»¤å¾®è°ƒæ•°æ®æ ¼å¼
{
    "instruction": "å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡",
    "input": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
    "output": "Machine learning is an important branch of artificial intelligence"
}

# å¯¹è¯æ ¼å¼ï¼ˆChatMLï¼‰
{
    "messages": [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç¿»è¯‘åŠ©æ‰‹"},
        {"role": "user", "content": "ç¿»è¯‘ï¼šä½ å¥½ä¸–ç•Œ"},
        {"role": "assistant", "content": "Hello World"}
    ]
}
```

**æ•°æ®è´¨é‡æœ€ä½³å®è·µ**:
- æ•°æ®æ¸…æ´—ï¼šå»é™¤å™ªå£°ã€é‡å¤ã€ä½è´¨é‡æ ·æœ¬
- æ•°æ®å¹³è¡¡ï¼šç¡®ä¿å„ç±»åˆ«æ ·æœ¬å‡è¡¡
- æ•°æ®å¤šæ ·æ€§ï¼šè¦†ç›–å„ç§åœºæ™¯å’Œè¾¹ç•Œæƒ…å†µ
- æ•°æ®éšç§ï¼šæ•æ„Ÿä¿¡æ¯è„±æ•å¤„ç†

**2. LoRA å¾®è°ƒå®ç°**

```python
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import SFTTrainer

# åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    load_in_8bit=True,  # é‡åŒ–åŠ è½½
    torch_dtype=torch.float16,
    device_map="auto"
)

# å‡†å¤‡æ¨¡å‹ç”¨äºè®­ç»ƒ
model = prepare_model_for_kbit_training(model)

# é…ç½® LoRA
lora_config = LoraConfig(
    r=16,  # LoRA ç§©ï¼Œé€šå¸¸ 8-64
    lora_alpha=32,  # ç¼©æ”¾å‚æ•°ï¼Œé€šå¸¸æ˜¯ r çš„ 2 å€
    target_modules=["q_proj", "v_proj"],  # ç›®æ ‡æ¨¡å—
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./lora_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    fp16=True,
    optim="paged_adamw_8bit"
)

# è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=training_args,
    tokenizer=tokenizer,
    max_seq_length=512
)
trainer.train()

# ä¿å­˜ LoRA æƒé‡
model.save_pretrained("./lora_model")
```

**3. QLoRA é«˜æ•ˆå¾®è°ƒ**

```python
from transformers import BitsAndBytesConfig

# 4-bit é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # 4-bit Normal Float
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True  # åµŒå¥—é‡åŒ–
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-13b-hf",
    quantization_config=bnb_config,
    device_map="auto"
)

# åç»­ LoRA é…ç½®åŒä¸Š
# QLoRA å¯åœ¨å•å¼  24GB GPU ä¸Šå¾®è°ƒ 13B æ¨¡å‹
```

**4. æ¨ç†ä¸åˆå¹¶**

```python
from peft import PeftModel

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

# åŠ è½½ LoRA æƒé‡
model = PeftModel.from_pretrained(base_model, "./lora_model")

# æ–¹å¼1ï¼šåŠ¨æ€åŠ è½½ï¼ˆLoRA æƒé‡å•ç‹¬å­˜å‚¨ï¼‰
# é€‚åˆå¤šé€‚é…å™¨åˆ‡æ¢åœºæ™¯

# æ–¹å¼2ï¼šåˆå¹¶æƒé‡ï¼ˆMerge and Unloadï¼‰
model = model.merge_and_unload()
# åˆå¹¶åä¿å­˜ä¸ºå®Œæ•´æ¨¡å‹
model.save_pretrained("./merged_model")
```

**5. DPOï¼ˆDirect Preference Optimizationï¼‰å®ç°**

```python
from trl import DPOTrainer
from peft import LoraConfig

# DPO æ•°æ®é›†æ ¼å¼
# æ¯æ¡æ•°æ®åŒ…å«ï¼šprompt, chosenï¼ˆåå¥½å›ç­”ï¼‰, rejectedï¼ˆä¸åå¥½å›ç­”ï¼‰

# é…ç½® LoRA ç”¨äº DPO
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# DPO è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./dpo_model",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=5e-7,  # DPO é€šå¸¸ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    fp16=True,
    optim="paged_adamw_8bit",
    beta=0.1  # DPO æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦
)

# åˆå§‹åŒ– DPO è®­ç»ƒå™¨
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=ref_model,  # å‚è€ƒæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ SFT åçš„æ¨¡å‹ï¼‰
    args=training_args,
    train_dataset=dpo_dataset,
    tokenizer=tokenizer,
    peft_config=lora_config,
    max_length=512,
    max_prompt_length=256
)

# å¼€å§‹è®­ç»ƒ
dpo_trainer.train()
dpo_trainer.save_model("./dpo_model")
```

**6. å¤šæ¨¡æ€å¾®è°ƒï¼ˆLLaVA é£æ ¼ï¼‰**

```python
# å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒæ•°æ®æ ¼å¼
{
    "id": "image_001",
    "image": "path/to/image.jpg",
    "conversations": [
        {
            "from": "human",
            "value": "<image>\næè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
        },
        {
            "from": "gpt",
            "value": "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€åº§é›ªå±±ï¼Œå±±é¡¶è¦†ç›–ç€çš‘çš‘ç™½é›ª..."
        }
    ]
}

# ä½¿ç”¨ LLaVA è®­ç»ƒè„šæœ¬
# https://github.com/haotian-liu/LLaVA
```

**7. PPOï¼ˆProximal Policy Optimizationï¼‰è®­ç»ƒ**

```python
from trl import PPOTrainer, PPOConfig
from transformers import AutoModelForCausalLMWithValueHead

# åˆå§‹åŒ–å¸¦ä»·å€¼å¤´çš„æ¨¡å‹
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "./sft_model",
    peft_config=lora_config
)

# PPO é…ç½®
ppo_config = PPOConfig(
    model_name="./sft_model",
    learning_rate=1.41e-5,
    batch_size=256,
    mini_batch_size=64,
    gradient_accumulation_steps=1,
    optimize_cuda_cache=True,
    early_stopping=False,
    target_kl=0.1,
    ppo_epochs=4,
    seed=42,
)

# åˆå§‹åŒ– PPO è®­ç»ƒå™¨
ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator
)

# è®­ç»ƒå¾ªç¯
for epoch in range(ppo_config.ppo_epochs):
    for batch in ppo_trainer.dataloader:
        queries = batch["query"]
        
        # ç”Ÿæˆå›ç­”
        response_tensors = ppo_trainer.generate(queries)
        
        # ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ‰“åˆ†
        rewards = reward_model(response_tensors)
        
        # PPO æ›´æ–°
        stats = ppo_trainer.step(queries, response_tensors, rewards)
        ppo_trainer.log_stats(stats, batch, rewards)
```

**8. å¥–åŠ±æ¨¡å‹è®­ç»ƒ**

```python
from transformers import AutoModelForSequenceClassification

# åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹
reward_model = AutoModelForSequenceClassification.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    num_labels=1,  # å›å½’ä»»åŠ¡
    torch_dtype=torch.bfloat16
)

# å¥–åŠ±æ¨¡å‹è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./reward_model",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    learning_rate=1e-5,
    warmup_ratio=0.1,
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=1000
)

# è®­ç»ƒå¥–åŠ±æ¨¡å‹
trainer = Trainer(
    model=reward_model,
    args=training_args,
    train_dataset=preference_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics
)
trainer.train()
```

**9. å¾®è°ƒæ•°æ®é›†æ„å»ºå·¥å…·**

```python
# ä½¿ç”¨ Self-Instruct æ–¹æ³•ç”ŸæˆæŒ‡ä»¤æ•°æ®
# https://github.com/tatsu-lab/stanford_alpaca

import json
import random
from openai import OpenAI

client = OpenAI()

# ç§å­ä»»åŠ¡
seed_tasks = [
    "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
    "ç¿»è¯‘ä»¥ä¸‹å¥å­ï¼šHello World"
]

# ç”Ÿæˆæ–°æŒ‡ä»¤
def generate_instruction(seed_task: str) -> dict:
    """åŸºäºç§å­ä»»åŠ¡ç”Ÿæˆæ–°çš„æŒ‡ä»¤-è¾“å‡ºå¯¹"""
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""åŸºäºä»¥ä¸‹ç¤ºä¾‹ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„æŒ‡ä»¤-è¾“å‡ºå¯¹ã€‚

ç¤ºä¾‹ï¼š
æŒ‡ä»¤ï¼š{seed_task}
è¾“å‡ºï¼š[å¯¹åº”çš„è¾“å‡º]

è¯·ç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„æŒ‡ä»¤ï¼ˆä¸ç¤ºä¾‹ä¸åŒï¼‰ï¼Œå¹¶æä¾›ç›¸åº”çš„è¾“å‡ºã€‚

æŒ‡ä»¤ï¼š"""
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©ç”Ÿæˆè®­ç»ƒæ•°æ®çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.8
    )
    
    # è§£æç”Ÿæˆçš„å†…å®¹
    content = response.choices[0].message.content
    # æå–æŒ‡ä»¤å’Œè¾“å‡º
    lines = content.strip().split('\n')
    instruction = lines[0].replace("æŒ‡ä»¤ï¼š", "").strip()
    output = '\n'.join(lines[2:]).replace("è¾“å‡ºï¼š", "").strip()
    
    return {
        "instruction": instruction,
        "input": "",
        "output": output
    }

# æ‰¹é‡ç”Ÿæˆ
dataset = []
for seed in seed_tasks:
    for _ in range(10):  # æ¯ä¸ªç§å­ç”Ÿæˆ10ä¸ªæ–°æŒ‡ä»¤
        try:
            new_item = generate_instruction(seed)
            dataset.append(new_item)
        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")

# ä¿å­˜æ•°æ®é›†
with open("generated_instructions.json", "w", encoding="utf-8") as f:
    json.dump(dataset, f, ensure_ascii=False, indent=2)
```

**10. å¾®è°ƒæ•ˆæœè¯„ä¼°**

```python
# ä½¿ç”¨ GPT-4 ä½œä¸ºè¯„ä¼°å™¨
from openai import OpenAI
import json

client = OpenAI()

def evaluate_response(instruction: str, output: str, expected: str) -> dict:
    """ä½¿ç”¨ GPT-4 è¯„ä¼°æ¨¡å‹è¾“å‡ºè´¨é‡"""
    
    eval_prompt = f"""è¯„ä¼°ä»¥ä¸‹æ¨¡å‹è¾“å‡ºçš„è´¨é‡ã€‚

æŒ‡ä»¤ï¼š{instruction}

æ¨¡å‹è¾“å‡ºï¼š{output}

å‚è€ƒç­”æ¡ˆï¼š{expected}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§ï¼šè¾“å‡ºå†…å®¹æ˜¯å¦æ­£ç¡®
2. å®Œæ•´æ€§ï¼šæ˜¯å¦è¦†ç›–æ‰€æœ‰è¦ç‚¹
3. æµç•…æ€§ï¼šè¯­è¨€æ˜¯å¦è‡ªç„¶æµç•…
4. æœ‰ç”¨æ€§ï¼šå¯¹ç”¨æˆ·æ˜¯å¦æœ‰å¸®åŠ©

ä»¥ JSON æ ¼å¼è¿”å›è¯„åˆ†å’Œç†ç”±ï¼š
{{
    "accuracy": åˆ†æ•°,
    "completeness": åˆ†æ•°,
    "fluency": åˆ†æ•°,
    "helpfulness": åˆ†æ•°,
    "overall": å¹³å‡åˆ†,
    "reasoning": "è¯„ä»·ç†ç”±"
}}"""
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ¨¡å‹è¯„ä¼°ä¸“å®¶ã€‚"},
            {"role": "user", "content": eval_prompt}
        ],
        temperature=0.3
    )
    
    # è§£æ JSON å“åº”
    result = response.choices[0].message.content
    try:
        return json.loads(result)
    except:
        return {"error": "è§£æå¤±è´¥", "raw": result}

# æ‰¹é‡è¯„ä¼°
def batch_evaluate(test_data: list) -> dict:
    """è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†"""
    results = []
    for item in test_data:
        score = evaluate_response(
            item["instruction"],
            item["model_output"],
            item["expected"]
        )
        results.append(score)
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    avg_scores = {
        "accuracy": sum(r["accuracy"] for r in results) / len(results),
        "completeness": sum(r["completeness"] for r in results) / len(results),
        "fluency": sum(r["fluency"] for r in results) / len(results),
        "helpfulness": sum(r["helpfulness"] for r in results) / len(results),
        "overall": sum(r["overall"] for r in results) / len(results)
    }
    
    return {
        "average_scores": avg_scores,
        "detailed_results": results
    }
```

### æœ€ä½³å®è·µ

**1. è¶…å‚æ•°è°ƒä¼˜**

| å‚æ•° | å»ºè®®å€¼ | è¯´æ˜ |
|------|--------|------|
| LoRA rank (r) | 8-64 | ä»»åŠ¡è¶Šå¤æ‚ï¼Œrank è¶Šé«˜ |
| lora_alpha | 2*r | ç¼©æ”¾ç³»æ•° |
| Learning Rate | 1e-4 ~ 5e-4 | é€šå¸¸æ¯”å…¨å‚æ•°å¾®è°ƒé«˜ 10 å€ |
| Batch Size | 4-16 | æ ¹æ®æ˜¾å­˜è°ƒæ•´ |
| Epochs | 3-10 | æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ |
| Max Length | 512-2048 | æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ |

**2. è®­ç»ƒç­–ç•¥**

**å­¦ä¹ ç‡è°ƒåº¦**:
```python
# ä½™å¼¦é€€ç« +  Warmup
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps
)
```

**æ¢¯åº¦æ£€æŸ¥ç‚¹**:
```python
model.gradient_checkpointing_enable()
# ä»¥è®¡ç®—æ¢æ˜¾å­˜ï¼Œå¯èŠ‚çœ 30-40% æ˜¾å­˜
```

**3. è¯„ä¼°ä¸ç›‘æ§**

```python
# è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡
from evaluate import load

bleu = load("bleu")
rouge = load("rouge")
perplexity = load("perplexity")

# è‡ªå®šä¹‰é¢†åŸŸè¯„ä¼°
# å»ºç«‹é¢†åŸŸç‰¹å®šçš„æµ‹è¯•é›†
# äººå·¥è¯„ä¼°æ ·æœ¬è´¨é‡
```

**4. ç¾éš¾æ€§é—å¿˜é˜²æŠ¤**

```python
# æ–¹å¼1ï¼šä¿ç•™éƒ¨åˆ†åŸå§‹æ•°æ®
mixed_dataset = original_data.sample(0.1) + new_domain_data

# æ–¹å¼2ï¼šEWC (Elastic Weight Consolidation)
# å¯¹é‡è¦å‚æ•°æ–½åŠ çº¦æŸ

# æ–¹å¼3ï¼šReplay Buffer
# å®šæœŸæ··åˆåŸå§‹ä»»åŠ¡æ ·æœ¬
```

**5. æ¨¡å‹åˆå¹¶æŠ€æœ¯**

```python
# ä½¿ç”¨ MergeKit åˆå¹¶å¤šä¸ª LoRA é€‚é…å™¨
# https://github.com/arcee-ai/mergekit

# çº¿æ€§åˆå¹¶
mergekit-yaml config.yaml ./merged_model

# é…ç½®ç¤ºä¾‹ (config.yaml)
# models:
#   - model: model_a
#     parameters:
#       weight: 0.6
#   - model: model_b
#     parameters:
#       weight: 0.4
# merge_method: linear
```

**6. æŒç»­é¢„è®­ç»ƒï¼ˆContinual Pre-trainingï¼‰**

```python
# é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡ŒæŒç»­é¢„è®­ç»ƒ
# é€‚ç”¨äºéœ€è¦å­¦ä¹ å¤§é‡é¢†åŸŸçŸ¥è¯†çš„åœºæ™¯

from transformers import DataCollatorForLanguageModeling

# å‡†å¤‡é¢†åŸŸè¯­æ–™
domain_corpus = load_dataset("text", data_files="domain_corpus.txt")

# é…ç½® MLM æ•°æ®æ•´ç†å™¨
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=True,  # æ©ç è¯­è¨€å»ºæ¨¡
    mlm_probability=0.15  # 15% çš„ token è¢«æ©ç 
)

# ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡å’Œæ›´é•¿çš„è®­ç»ƒæ­¥æ•°
training_args = TrainingArguments(
    output_dir="./continual_pretrained",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    learning_rate=1e-5,  # æ¯”å¾®è°ƒæ›´å¤§çš„å­¦ä¹ ç‡
    warmup_steps=1000,
    save_steps=10000,
    logging_steps=100
)

# è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)
trainer.train()
```

**7. æ•°æ®å¢å¼ºç­–ç•¥**

```python
# ä½¿ç”¨ back-translation è¿›è¡Œæ•°æ®å¢å¼º
from transformers import pipeline

# åˆå§‹åŒ–ç¿»è¯‘æ¨¡å‹
translator_en_zh = pipeline("translation", model="Helsinki-NLP/opus-mt-en-zh")
translator_zh_en = pipeline("translation", model="Helsinki-NLP/opus-mt-zh-en")

def back_translate(text: str, src_lang: str = "zh") -> str:
    """å›è¯‘æ•°æ®å¢å¼º"""
    if src_lang == "zh":
        # ä¸­æ–‡ -> è‹±æ–‡ -> ä¸­æ–‡
        en_text = translator_zh_en(text)[0]["translation_text"]
        back_text = translator_en_zh(en_text)[0]["translation_text"]
    else:
        # è‹±æ–‡ -> ä¸­æ–‡ -> è‹±æ–‡
        zh_text = translator_en_zh(text)[0]["translation_text"]
        back_text = translator_zh_en(zh_text)[0]["translation_text"]
    return back_text

# ä½¿ç”¨ EDAï¼ˆEasy Data Augmentationï¼‰
import random

def eda_augment(text: str, n_sr: int = 2, n_ri: int = 2) -> list:
    """
    Easy Data Augmentation
    - SR: åŒä¹‰è¯æ›¿æ¢
    - RI: éšæœºæ’å…¥
    """
    augmented = [text]
    
    # åŒä¹‰è¯æ›¿æ¢
    for _ in range(n_sr):
        words = text.split()
        idx = random.randint(0, len(words) - 1)
        # æ›¿æ¢ä¸ºåŒä¹‰è¯ï¼ˆéœ€è¦é¢„å®šä¹‰åŒä¹‰è¯è¯å…¸ï¼‰
        # words[idx] = synonym
        augmented.append(" ".join(words))
    
    return augmented
```

**8. æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–**

```python
# DeepSpeed ZeRO é…ç½®
# ds_config.json
{
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2,
        "offload_optimizer": {
            "device": "cpu",
            "pin_memory": true
        },
        "allgather_partitions": true,
        "allgather_bucket_size": 2e8,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2e8,
        "contiguous_gradients": true
    },
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto"
}

# ä½¿ç”¨ DeepSpeed è®­ç»ƒ
from accelerate import Accelerator

accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin)
model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)
```

### å¸¸è§é™·é˜±

**1. æ•°æ®æ³„éœ²**
- âŒ æµ‹è¯•æ•°æ®æ··å…¥è®­ç»ƒé›†
- âœ… ä¸¥æ ¼åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
- âœ… ä½¿ç”¨æ—¶é—´æˆ³åˆ†å‰²æ—¶åºæ•°æ®

**2. è¿‡æ‹Ÿåˆ**
- âŒ è®­ç»ƒæ•°æ®å¤ªå°‘æˆ– epochs è¿‡å¤š
- âœ… ä½¿ç”¨éªŒè¯é›†æ—©åœ
- âœ… æ·»åŠ  Dropout å’Œæƒé‡è¡°å‡
- âœ… æ•°æ®å¢å¼ºå’Œæ­£åˆ™åŒ–

**3. ç¾éš¾æ€§é—å¿˜**
- âŒ å®Œå…¨é—å¿˜é€šç”¨èƒ½åŠ›
- âœ… æ··åˆä¿ç•™åŸå§‹èƒ½åŠ›çš„æ ·æœ¬
- âœ… ä½¿ç”¨ Adapter ç­‰æ¨¡å—åŒ–æ–¹æ³•

**4. é‡åŒ–ç²¾åº¦æŸå¤±**
- âŒ è¿‡åº¦é‡åŒ–å¯¼è‡´æ•ˆæœä¸‹é™
- âœ… 8-bit é€šå¸¸æ˜¯ç²¾åº¦ä¸æ•ˆç‡çš„å¹³è¡¡ç‚¹
- âœ… å…³é”®å±‚ä¿æŒ FP16/FP32

**5. æ•°æ®è´¨é‡é™·é˜±**
- âŒ ä½¿ç”¨æœªæ¸…æ´—çš„åŸå§‹æ•°æ®
- âœ… å»ºç«‹æ•°æ®è´¨é‡è¯„ä¼°æµç¨‹
- âœ… å®æ–½æ•°æ®å»é‡å’Œè¿‡æ»¤

**6. è¶…å‚æ•°é€‰æ‹©**
- âŒ ä½¿ç”¨é»˜è®¤å‚æ•°ä¸è°ƒä¼˜
- âœ… ä½¿ç”¨ wandb æˆ– tensorboard è¿›è¡Œå®éªŒè¿½è¸ª
- âœ… ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–å¯»æ‰¾æœ€ä¼˜å‚æ•°

**7. æ¨ç†æ—¶çš„ä¸ä¸€è‡´**
- âŒ è®­ç»ƒæ—¶ä½¿ç”¨ç‰¹å®šæ ¼å¼ï¼Œæ¨ç†æ—¶æ ¼å¼ä¸ä¸€è‡´
- âœ… ç¡®ä¿æ¨ç†æ—¶åº”ç”¨ç›¸åŒçš„èŠå¤©æ¨¡æ¿
- âœ… éªŒè¯æ¨ç†å‚æ•°ï¼ˆtemperature, top_pï¼‰çš„ä¸€è‡´æ€§

**8. åˆ†å¸ƒå¼è®­ç»ƒé™·é˜±**
- âŒ å¿½ç•¥æ¢¯åº¦åŒæ­¥é—®é¢˜
- âœ… ä½¿ç”¨ DistributedDataParallel
- âœ… æ³¨æ„éšæœºç§å­çš„è®¾ç½®

## ğŸ“š èµ„æºç´¢å¼•

### å­¦æœ¯è®ºæ–‡

1. **LoRA: Low-Rank Adaptation of Large Language Models** (2021)
   - ä½œè€…ï¼šEdward Hu et al., Microsoft
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2106.09685
   - æ ¸å¿ƒè´¡çŒ®ï¼šæå‡ºä½ç§©é€‚é…æ–¹æ³•ï¼Œå¤§å¹…é™ä½å¾®è°ƒå‚æ•°é‡

2. **QLoRA: Efficient Finetuning of Quantized LLMs** (2023)
   - ä½œè€…ï¼šTim Dettmers et al., UW
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2305.14314
   - æ ¸å¿ƒè´¡çŒ®ï¼š4-bit é‡åŒ– + LoRAï¼Œå•å¡å¾®è°ƒ 65B æ¨¡å‹

3. **Parameter-Efficient Transfer Learning for NLP** (2019)
   - ä½œè€…ï¼šNeil Houlsby et al., Google
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/1902.00751
   - æ ¸å¿ƒè´¡çŒ®ï¼šAdapter å±‚æ–¹æ³•

4. **Training language models to follow instructions with human feedback** (2022)
   - ä½œè€…ï¼šLong Ouyang et al., OpenAI
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2203.02155
   - æ ¸å¿ƒè´¡çŒ®ï¼šInstructGPT çš„ RLHF è®­ç»ƒæ–¹æ³•

5. **Llama 2: Open Foundation and Fine-Tuned Chat Models** (2023)
   - ä½œè€…ï¼šHugo Touvron et al., Meta
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2307.09288
   - æ ¸å¿ƒè´¡çŒ®ï¼šå¼€æºå¯å•†ç”¨çš„å¤§æ¨¡å‹åŠå¾®è°ƒæ–¹æ³•

6. **Direct Preference Optimization: Your Language Model is Secretly a Reward Model** (2023)
   - ä½œè€…ï¼šRafael Rafailov et al., Stanford
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2305.18290
   - æ ¸å¿ƒè´¡çŒ®ï¼šDPO ç®—æ³•ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹å³å¯è¿›è¡Œäººç±»åå¥½ä¼˜åŒ–

7. **Full Parameter Fine-tuning for Large Language Models with Limited Resources** (2023)
   - ä½œè€…ï¼šKai Lv et al., Fudan University
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2306.09782
   - æ ¸å¿ƒè´¡çŒ®ï¼šLISA æ–¹æ³•ï¼Œæœ‰é™èµ„æºä¸‹çš„å…¨å‚æ•°å¾®è°ƒ

8. **Scaling Laws for Reward Model Overoptimization** (2022)
   - ä½œè€…ï¼šLeo Gao et al., Anthropic
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/2210.10760
   - æ ¸å¿ƒè´¡çŒ®ï¼šæ·±å…¥åˆ†æ RLHF ä¸­çš„å¥–åŠ±æ¨¡å‹ä¼˜åŒ–é—®é¢˜

### æŠ€æœ¯æ–‡æ¡£

1. **Hugging Face PEFT æ–‡æ¡£**
   - https://huggingface.co/docs/peft
   - å‚æ•°é«˜æ•ˆå¾®è°ƒçš„å®˜æ–¹å®ç°æŒ‡å—

2. **Hugging Face TRL æ–‡æ¡£**
   - https://huggingface.co/docs/trl
   - Transformer å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶

3. **Llama-Recipes**
   - https://github.com/meta-llama/llama-recipes
   - Meta å®˜æ–¹çš„ Llama å¾®è°ƒç¤ºä¾‹

4. **Axolotl**
   - https://github.com/OpenAccess-AI-Collective/axolotl
   - ç®€åŒ–å¤§æ¨¡å‹å¾®è°ƒçš„å·¥å…·

5. **Hugging Face Alignment Handbook**
   - https://github.com/huggingface/alignment-handbook
   - å¯¹é½æŠ€æœ¯çš„å®Œæ•´æŒ‡å—

6. **DeepSpeed æ–‡æ¡£**
   - https://www.deepspeed.ai/
   - å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶

### å¼€æºé¡¹ç›®

1. **LLaMA-Factory**
   - https://github.com/hiyouga/LLaMA-Factory
   - ä¸€ç«™å¼å¤§æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œæ”¯æŒ 100+ æ¨¡å‹

2. **Unsloth**
   - https://github.com/unslothai/unsloth
   - 2-5 å€æ›´å¿«çš„å¾®è°ƒï¼Œ50% æ›´å°‘æ˜¾å­˜

3. **Xtuner**
   - https://github.com/InternLM/xtuner
   - ä¹¦ç”ŸÂ·æµ¦è¯­çš„é«˜æ•ˆå¾®è°ƒå·¥å…·ç®±

4. **Modal**
   - https://modal.com/
   - äº‘ç«¯å¾®è°ƒéƒ¨ç½²å¹³å°

5. **Firefly**
   - https://github.com/yangjianxin1/Firefly
   - ä¸­æ–‡å¤§æ¨¡å‹å¾®è°ƒå·¥å…·

6. **SWIFT**
   - https://github.com/modelscope/swift
   - é˜¿é‡Œå·´å·´ ModelScope è½»é‡çº§å¾®è°ƒæ¡†æ¶

7. **MergeKit**
   - https://github.com/arcee-ai/mergekit
   - æ¨¡å‹åˆå¹¶å·¥å…·åŒ…

### æ•°æ®é›†èµ„æº

1. **Awesome Instruction Datasets**
   - https://github.com/01-ai/Yi-1.5#training-data
   - æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†é›†åˆ

2. **Hugging Face Datasets**
   - https://huggingface.co/datasets?task_categories=task_categories:text-generation
   - ç¤¾åŒºå…±äº«çš„æ–‡æœ¬ç”Ÿæˆæ•°æ®é›†

3. **ShareGPT**
   - https://sharegpt.com/
   - çœŸå®å¯¹è¯æ•°æ®

4. **UltraChat**
   - https://github.com/thunlp/UltraChat
   - å¤§è§„æ¨¡é«˜è´¨é‡å¤šè½®å¯¹è¯æ•°æ®

5. **CodeAlpaca**
   - https://github.com/sahil280114/codealpaca
   - ä»£ç ç”ŸæˆæŒ‡ä»¤æ•°æ®é›†

## ğŸ”— å…³è”çŸ¥è¯†

```mermaid
graph TD
    C02[C02_Model_Fine-Tuning]
    
    C02 --> C01[C01_Prompt_Engineering]
    C02 --> C03[C03_LLMOps]
    
    C02 -.-> A03[A03_Design_Architecture/B03_Data_Storage]
    C02 -.-> A01[A01_Infrastructure/B10_Cloud_Platforms]
    C02 -.-> A02[A02_Engineering_Processes/B02_Technical_Practices]
    
    C01 --> |æç¤ºä¼˜åŒ–æŒ‡å¯¼å¾®è°ƒç›®æ ‡| C02
    C02 --> |å¾®è°ƒåæ¨¡å‹éƒ¨ç½²| C03
```

## ğŸ’¡ å­¦ä¹ å»ºè®®

### å‰ç½®çŸ¥è¯†
- æ·±åº¦å­¦ä¹ åŸºç¡€ï¼ˆåå‘ä¼ æ’­ã€ä¼˜åŒ–å™¨ï¼‰
- Transformer æ¶æ„åŸç†
- PyTorch åŸºç¡€æ“ä½œ
- CUDA å’Œ GPU ç¼–ç¨‹åŸºç¡€

### å­¦ä¹ è·¯å¾„

**ç¬¬1å‘¨ï¼šç†è®ºç†è§£**
- å­¦ä¹  LoRA/QLoRA è®ºæ–‡
- ç†è§£ PEFT åŸç†
- äº†è§£ RLHF æµç¨‹

**ç¬¬2å‘¨ï¼šç¯å¢ƒæ­å»º**
- é…ç½® GPU ç¯å¢ƒ
- å®‰è£… Transformers + PEFT
- è·‘é€šå®˜æ–¹ç¤ºä¾‹

**ç¬¬3å‘¨ï¼šå®è·µå¾®è°ƒ**
- ä½¿ç”¨å…¬å¼€æ•°æ®é›†å¾®è°ƒå°æ¨¡å‹
- å°è¯•ä¸åŒ PEFT æ–¹æ³•å¯¹æ¯”
- å­¦ä¹ è¶…å‚æ•°è°ƒä¼˜

**ç¬¬4å‘¨ï¼šé¡¹ç›®å®æˆ˜**
- æ„å»ºé¢†åŸŸæ•°æ®é›†
- å®Œæ•´å¾®è°ƒæµç¨‹
- æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²

### å®è·µé¡¹ç›®

**é¡¹ç›®1ï¼šé¢†åŸŸé—®ç­”åŠ©æ‰‹**
- æ•°æ®ï¼šé¢†åŸŸæ–‡æ¡£ + é—®ç­”å¯¹
- æ¨¡å‹ï¼šLlama-2-7B + LoRA
- è¾“å‡ºï¼šé¢†åŸŸä¸“å± Chatbot

**é¡¹ç›®2ï¼šä»£ç ç”Ÿæˆæ¨¡å‹**
- æ•°æ®ï¼šGitHub ä»£ç  + æ³¨é‡Š
- æ¨¡å‹ï¼šCodeLlama + QLoRA
- è¾“å‡ºï¼šIDE ä»£ç è¡¥å…¨æ’ä»¶

**é¡¹ç›®3ï¼šå¤šè¯­è¨€é€‚é…**
- æ•°æ®ï¼šä½èµ„æºè¯­è¨€è¯­æ–™
- æ¨¡å‹ï¼šmBERT/XLM + å¾®è°ƒ
- è¾“å‡ºï¼šå¤šè¯­è¨€ NLP æœåŠ¡

**é¡¹ç›®4ï¼šä¸ªæ€§åŒ–å†™ä½œåŠ©æ‰‹**
- æ•°æ®ï¼šç‰¹å®šä½œè€…æ–‡æœ¬ + å†™ä½œé£æ ¼ç¤ºä¾‹
- æ¨¡å‹ï¼šGPT-J/Neo + LoRA
- è¾“å‡ºï¼šé£æ ¼è¿ç§»å†™ä½œå·¥å…·

**é¡¹ç›®5ï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒ**
- æ•°æ®ï¼šäººå·¥æ ‡æ³¨çš„åå¥½æ•°æ®
- æ¨¡å‹ï¼šBERT/RoBERTa ä½œä¸ºåŸºç¡€
- è¾“å‡ºï¼šç”¨äº RLHF çš„å¥–åŠ±æ¨¡å‹

## ğŸ”„ ç»´æŠ¤è¯´æ˜

- **æ›´æ–°é¢‘ç‡**: æ¯å­£åº¦è·Ÿè¸ªæœ€æ–°å¾®è°ƒæŠ€æœ¯
- **è´¨é‡æ ‡å‡†**: ä»£ç å¯è¿è¡Œï¼Œè®ºæ–‡é“¾æ¥æœ‰æ•ˆ
- **è´¡çŒ®æ–¹å¼**: æäº¤æ–°çš„å¾®è°ƒæ–¹æ³•ã€ä¼˜åŒ–æŠ€å·§ã€å®è·µæ¡ˆä¾‹
