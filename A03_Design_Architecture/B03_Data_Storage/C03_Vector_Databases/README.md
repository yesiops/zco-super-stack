# C03 Vector Databases

**æ‰€å±å­é¢†åŸŸ**: [B03_Data_Storage](../README.md)  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-30  
**æœ€åæ›´æ–°**: 2026-01-30

## ğŸ“‹ ä¸»é¢˜å®šä½

å‘é‡æ•°æ®åº“ï¼ˆVector Databaseï¼‰æ˜¯ä¸“ä¸ºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡æ•°æ®è€Œè®¾è®¡çš„æ•°æ®åº“ç³»ç»Ÿã€‚é€šè¿‡è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰æœç´¢ç®—æ³•ï¼Œå®ç°é«˜æ•ˆçš„è¯­ä¹‰ç›¸ä¼¼æ€§æ£€ç´¢ï¼Œæ˜¯ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ã€æ¨èç³»ç»Ÿã€å›¾åƒæ£€ç´¢ç­‰ AI åº”ç”¨çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### åŸºæœ¬å®šä¹‰

**å‘é‡åµŒå…¥ï¼ˆVector Embeddingï¼‰**: å°†éç»“æ„åŒ–æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰è½¬æ¢ä¸ºé«˜ç»´æ•°å€¼å‘é‡çš„è¿‡ç¨‹ã€‚è¯­ä¹‰ç›¸ä¼¼çš„æ•°æ®åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»ç›¸è¿‘ã€‚

**å‘é‡æ£€ç´¢**: åœ¨å‘é‡ç©ºé—´ä¸­æŸ¥æ‰¾ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„å‘é‡çš„è¿‡ç¨‹ï¼Œé€šå¸¸ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§æ°è·ç¦»æˆ–ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡ã€‚

**è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰**: ç‰ºç‰²å°‘é‡ç²¾åº¦æ¢å–æ˜¾è‘—æ€§èƒ½æå‡çš„æ£€ç´¢ç®—æ³•ï¼ŒåŒ…æ‹¬ HNSWã€IVFã€PQ ç­‰ç´¢å¼•ç»“æ„ã€‚

### æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ | ä¼˜åŠ¿ |
|------|------|------|
| **è¯­ä¹‰æ£€ç´¢** | åŸºäºæ„ä¹‰è€Œéå…³é”®è¯åŒ¹é… | ç†è§£åŒä¹‰è¯ã€å¤šä¹‰è¯ |
| **å¤šæ¨¡æ€æ”¯æŒ** | ç»Ÿä¸€è¡¨ç¤ºæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ | è·¨æ¨¡æ€æ£€ç´¢ |
| **é«˜ç»´å¤„ç†** | æ”¯æŒ 768-4096 ç»´å‘é‡ | é€‚é…ç°ä»£åµŒå…¥æ¨¡å‹ |
| **å®æ—¶æ›´æ–°** | åŠ¨æ€å¢åˆ å‘é‡æ•°æ® | æ”¯æŒæµå¼åœºæ™¯ |
| **æ··åˆæŸ¥è¯¢** | å‘é‡ç›¸ä¼¼åº¦ + å…ƒæ•°æ®è¿‡æ»¤ | ç²¾ç¡®æ§åˆ¶ç»“æœ |

### ç›¸ä¼¼åº¦åº¦é‡

| åº¦é‡æ–¹å¼ | å…¬å¼ | é€‚ç”¨åœºæ™¯ |
|----------|------|----------|
| **ä½™å¼¦ç›¸ä¼¼åº¦** | $\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}$ | å…³æ³¨æ–¹å‘è€Œéå¹…åº¦ï¼ˆæ–‡æœ¬è¯­ä¹‰ï¼‰ |
| **æ¬§æ°è·ç¦»** | $d = \sqrt{\sum(A_i - B_i)^2}$ | å…³æ³¨ç»å¯¹ä½ç½®ï¼ˆå›¾åƒç‰¹å¾ï¼‰ |
| **ç‚¹ç§¯** | $A \cdot B = \sum A_i B_i$ | è€ƒè™‘å‘é‡å¹…åº¦ï¼ˆæ¨èç³»ç»Ÿï¼‰ |
| **æ±‰æ˜è·ç¦»** | ä¸åŒä½çš„æ•°é‡ | äºŒè¿›åˆ¶å‘é‡ï¼ˆå›¾åƒå“ˆå¸Œï¼‰ |

### ç´¢å¼•ç®—æ³•

**1. HNSW (Hierarchical Navigable Small World)**
- å›¾ç»“æ„ç´¢å¼•ï¼Œæ„å»ºå¤šå±‚å¯¼èˆªå›¾
- é«˜å¬å›ç‡ã€å¿«é€ŸæŸ¥è¯¢
- å†…å­˜æ¶ˆè€—è¾ƒå¤§
- ä»£è¡¨ï¼šFAISS-HNSWã€Milvusã€Pinecone

**2. IVF (Inverted File Index)**
- å‘é‡ç©ºé—´èšç±»ï¼Œå€’æ’ç´¢å¼•
- å¹³è¡¡æ€§èƒ½å’Œèµ„æº
- ä»£è¡¨ï¼šFAISS-IVF

**3. PQ (Product Quantization)**
- å‘é‡å‹ç¼©æŠ€æœ¯
- å¤§å¹…é™ä½å†…å­˜å ç”¨
- ä»£è¡¨ï¼šFAISS-PQã€ScaNN

**4. æš´åŠ›æœç´¢ï¼ˆFlatï¼‰**
- ç²¾ç¡®è®¡ç®—ï¼Œæ— ç´¢å¼•
- æ•°æ®é‡å°æ—¶ä½¿ç”¨
- 100% å¬å›ç‡

### åº”ç”¨åœºæ™¯

- **RAG ç³»ç»Ÿ**: çŸ¥è¯†åº“è¯­ä¹‰æ£€ç´¢
- **æ¨èç³»ç»Ÿ**: ç›¸ä¼¼å•†å“/å†…å®¹æ¨è
- **å›¾åƒæ£€ç´¢**: ä»¥å›¾æœå›¾
- **å¼‚å¸¸æ£€æµ‹**: å‘é‡ç©ºé—´ç¦»ç¾¤ç‚¹æ£€æµ‹
- **è¯­ä¹‰æœç´¢**: æ–‡æ¡£/ä»£ç æœç´¢

## ğŸ› ï¸ æŠ€æœ¯å®è·µ

### å®ç°æ–¹æ³•

**1. ä½¿ç”¨ Chromaï¼ˆè½»é‡çº§æœ¬åœ°å‘é‡åº“ï¼‰**

```python
import chromadb
from chromadb.utils import embedding_functions

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = chromadb.PersistentClient(path="./chroma_db")

# ä½¿ç”¨ OpenAI åµŒå…¥æ¨¡å‹
openai_ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key="your-api-key",
    model_name="text-embedding-3-small"
)

# åˆ›å»ºé›†åˆï¼ˆç›¸å½“äºè¡¨ï¼‰
collection = client.create_collection(
    name="documents",
    embedding_function=openai_ef,
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
)

# æ·»åŠ æ–‡æ¡£
documents = [
    "å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨å­˜å‚¨å’Œæ£€ç´¢å‘é‡æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿ",
    "RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆå‘é‡æ£€ç´¢å’Œè¯­è¨€æ¨¡å‹",
    "åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡è¡¨ç¤º",
]

ids = ["doc1", "doc2", "doc3"]
metadatas = [
    {"source": "tech_doc", "category": "database"},
    {"source": "tech_doc", "category": "ai"},
    {"source": "tech_doc", "category": "ml"},
]

collection.add(
    documents=documents,
    ids=ids,
    metadatas=metadatas
)

# è¯­ä¹‰æ£€ç´¢
results = collection.query(
    query_texts=["ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“"],
    n_results=3,
    where={"category": "database"}  # å…ƒæ•°æ®è¿‡æ»¤
)

print(results['documents'])
print(results['distances'])  # ç›¸ä¼¼åº¦è·ç¦»
print(results['metadatas'])
```

**2. ä½¿ç”¨ FAISSï¼ˆFacebook AI ç›¸ä¼¼æ€§æœç´¢ï¼‰**

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# åŠ è½½åµŒå…¥æ¨¡å‹
model = SentenceTransformer('BAAI/bge-large-zh-v1.5')

# å‡†å¤‡æ•°æ®
documents = [
    "å‘é‡æ•°æ®åº“æ”¯æŒé«˜æ•ˆçš„è¯­ä¹‰æ£€ç´¢",
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯",
    "Python æ˜¯æ•°æ®ç§‘å­¦çš„ä¸»æµè¯­è¨€",
    "ç¥ç»ç½‘ç»œç”¨äºæ·±åº¦å­¦ä¹ å’Œæ¨¡å¼è¯†åˆ«",
]

# ç”Ÿæˆå‘é‡åµŒå…¥
dimension = 1024  # bge-large çš„ç»´åº¦
embeddings = model.encode(documents)
embeddings = np.array(embeddings).astype('float32')

# åˆ›å»ºç´¢å¼•
# æ–¹å¼1ï¼šæš´åŠ›æœç´¢ï¼ˆå°æ•°æ®é›†ï¼‰
index_flat = faiss.IndexFlatIP(dimension)  # å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦éœ€å½’ä¸€åŒ–ï¼‰
index_flat.add(embeddings)

# æ–¹å¼2ï¼šHNSW ç´¢å¼•ï¼ˆå¤§æ•°æ®é›†ï¼‰
index_hnsw = faiss.IndexHNSWFlat(dimension, 32)  # 32 é‚»å±…
index_hnsw.hnsw.efConstruction = 200  # æ„å»ºæ—¶æœç´¢æ·±åº¦
index_hnsw.add(embeddings)

# æ–¹å¼3ï¼šIVF ç´¢å¼•ï¼ˆå¹³è¡¡æ–¹æ¡ˆï¼‰
nlist = 100  # èšç±»ä¸­å¿ƒæ•°
quantizer = faiss.IndexFlatIP(dimension)
index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)
index_ivf.train(embeddings)  # è®­ç»ƒèšç±»
index_ivf.add(embeddings)
index_ivf.nprobe = 10  # æŸ¥è¯¢æ—¶æœç´¢çš„èšç±»æ•°

# æŸ¥è¯¢
query = "è¯­ä¹‰æœç´¢æŠ€æœ¯"
query_vector = model.encode([query])
query_vector = np.array(query_vector).astype('float32')

# å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
faiss.normalize_L2(query_vector)
faiss.normalize_L2(embeddings)

# æœç´¢
k = 3  # è¿”å› top-3
distances, indices = index_hnsw.search(query_vector, k)

print("æŸ¥è¯¢:", query)
for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
    print(f"{i+1}. [{dist:.4f}] {documents[idx]}")

# ä¿å­˜å’ŒåŠ è½½ç´¢å¼•
faiss.write_index(index_hnsw, "my_index.faiss")
loaded_index = faiss.read_index("my_index.faiss")
```

**3. ä½¿ç”¨ Pineconeï¼ˆæ‰˜ç®¡äº‘æœåŠ¡ï¼‰**

```python
from pinecone import Pinecone, ServerlessSpec

# åˆå§‹åŒ–
pc = Pinecone(api_key="your-api-key")

# åˆ›å»ºç´¢å¼•
index_name = "document-search"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # OpenAI text-embedding-3-small
        metric="cosine",
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

# è¿æ¥ç´¢å¼•
index = pc.Index(index_name)

# æ’å…¥å‘é‡
vectors = [
    {
        "id": "vec1",
        "values": [0.1, 0.2, 0.3, ...],  # 1536 ç»´å‘é‡
        "metadata": {
            "source": "article1",
            "category": "tech",
            "created_at": "2024-01-30"
        }
    },
    # ... æ›´å¤šå‘é‡
]

index.upsert(vectors=vectors, namespace="ns1")

# æŸ¥è¯¢
query_vector = [0.1, 0.2, 0.3, ...]
results = index.query(
    vector=query_vector,
    top_k=5,
    namespace="ns1",
    filter={
        "category": {"$eq": "tech"},
        "created_at": {"$gte": "2024-01-01"}
    },
    include_metadata=True
)

for match in results['matches']:
    print(f"ID: {match['id']}, Score: {match['score']:.4f}")
```

**4. ä½¿ç”¨ Milvus/Zillizï¼ˆåˆ†å¸ƒå¼å‘é‡æ•°æ®åº“ï¼‰**

```python
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection

# è¿æ¥
connections.connect("default", host="localhost", port="19530")

# å®šä¹‰å­—æ®µ
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=128),
]

# åˆ›å»ºé›†åˆ
schema = CollectionSchema(fields, "Document collection")
collection = Collection("documents", schema)

# åˆ›å»ºç´¢å¼•
index_params = {
    "index_type": "HNSW",  # æˆ– "IVF_FLAT", "DISKANN"
    "metric_type": "COSINE",
    "params": {
        "M": 16,  # HNSW å‚æ•°
        "efConstruction": 200
    }
}
collection.create_index(field_name="embedding", index_params=index_params)

# æ’å…¥æ•°æ®
import random
entities = [
    [i for i in range(100)],  # id
    [[random.random() for _ in range(768)] for _ in range(100)],  # embedding
    [f"Document {i}" for i in range(100)],  # text
    ["tech" if i % 2 == 0 else "business" for i in range(100)],  # category
]
collection.insert(entities)

# åŠ è½½é›†åˆåˆ°å†…å­˜
collection.load()

# æœç´¢
search_params = {
    "metric_type": "COSINE",
    "params": {"ef": 64}  # HNSW æœç´¢å‚æ•°
}

results = collection.search(
    data=[[random.random() for _ in range(768)]],
    anns_field="embedding",
    param=search_params,
    limit=5,
    expr='category == "tech"',  # æ ‡é‡è¿‡æ»¤
    output_fields=["text", "category"]
)

for result in results:
    for item in result:
        print(f"ID: {item.id}, Distance: {item.distance}, Text: {item.entity.text}")
```

**5. RAG ç³»ç»Ÿå®Œæ•´å®ç°**

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

class RAGSystem:
    """æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ"""
    
    def __init__(self, openai_api_key: str, persist_dir: str = "./chroma_db"):
        self.embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        self.persist_dir = persist_dir
        self.vectorstore = None
        self.qa_chain = None
        
    def ingest_documents(self, documents: List[str], metadata: List[dict] = None):
        """æ–‡æ¡£æ‘„å…¥"""
        # æ–‡æœ¬åˆ‡åˆ†
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " "]
        )
        
        chunks = []
        for i, doc in enumerate(documents):
            splits = text_splitter.split_text(doc)
            for j, split in enumerate(splits):
                meta = metadata[i].copy() if metadata else {}
                meta.update({"chunk_index": j, "source_doc": i})
                chunks.append({"text": split, "metadata": meta})
        
        # å­˜å…¥å‘é‡åº“
        texts = [c["text"] for c in chunks]
        metadatas = [c["metadata"] for c in chunks]
        
        self.vectorstore = Chroma.from_texts(
            texts=texts,
            embedding=self.embeddings,
            metadatas=metadatas,
            persist_directory=self.persist_dir
        )
        
    def setup_qa_chain(self):
        """è®¾ç½®é—®ç­”é“¾"""
        if not self.vectorstore:
            self.vectorstore = Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        
        # è‡ªå®šä¹‰æç¤ºè¯
        RAG_PROMPT = """åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”
2. å›ç­”è¦å‡†ç¡®ã€ç®€æ´
3. å¦‚æœ‰å¼•ç”¨ï¼Œè¯·æ³¨æ˜æ¥æº

å›ç­”ï¼š"""
        
        prompt = PromptTemplate(
            template=RAG_PROMPT,
            input_variables=["context", "question"]
        )
        
        # æ£€ç´¢å™¨é…ç½®
        retriever = self.vectorstore.as_retriever(
            search_type="mmr",  # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
            search_kwargs={
                "k": 5,  # æ£€ç´¢æ•°é‡
                "fetch_k": 20,  # MMR å€™é€‰æ± 
                "lambda_mult": 0.5  # å¤šæ ·æ€§å¹³è¡¡
            }
        )
        
        # æ„å»º QA é“¾
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0,
                openai_api_key=self.embeddings.openai_api_key
            ),
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
    
    def query(self, question: str) -> dict:
        """æŸ¥è¯¢"""
        if not self.qa_chain:
            self.setup_qa_chain()
        
        result = self.qa_chain({"query": question})
        
        return {
            "answer": result["result"],
            "sources": [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ]
        }

# ä½¿ç”¨ç¤ºä¾‹
rag = RAGSystem(openai_api_key="your-key")

# æ‘„å…¥æ–‡æ¡£
docs = [
    open("doc1.txt").read(),
    open("doc2.txt").read(),
]
rag.ingest_documents(docs)

# æŸ¥è¯¢
response = rag.query("å‘é‡æ•°æ®åº“çš„ä¸»è¦åº”ç”¨åœºæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response["answer"])
```

### æœ€ä½³å®è·µ

**1. åµŒå…¥æ¨¡å‹é€‰æ‹©**

| æ¨¡å‹ | ç»´åº¦ | è¯­è¨€ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| text-embedding-3-small | 1536 | å¤šè¯­è¨€ | é€šç”¨åœºæ™¯ï¼Œæˆæœ¬æ•æ„Ÿ |
| text-embedding-3-large | 3072 | å¤šè¯­è¨€ | é«˜ç²¾åº¦éœ€æ±‚ |
| BAAI/bge-large-zh | 1024 | ä¸­æ–‡ | ä¸­æ–‡è¯­ä¹‰æ£€ç´¢ |
| BAAI/bge-m3 | 1024 | å¤šè¯­è¨€ | å¤šè¯­è¨€æ··åˆåœºæ™¯ |
| E5-Mistral-7B | 4096 | å¤šè¯­è¨€ | å¤æ‚è¯­ä¹‰ç†è§£ |

**2. åˆ†å—ç­–ç•¥**

```python
# åŸºäºæ–‡æœ¬ç»“æ„çš„æ™ºèƒ½åˆ†å—
def semantic_chunking(text: str, max_chunk_size: int = 1000) -> List[str]:
    """åŸºäºè¯­ä¹‰çš„åˆ†å—ç­–ç•¥"""
    
    # 1. æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        # æ®µè½è¿‡é•¿åˆ™æŒ‰å¥å­åˆ†å‰²
        if len(para) > max_chunk_size:
            sentences = para.split('ã€‚')
            for sent in sentences:
                if len(current_chunk) + len(sent) < max_chunk_size:
                    current_chunk += sent + "ã€‚"
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = sent + "ã€‚"
        else:
            if len(current_chunk) + len(para) < max_chunk_size:
                current_chunk += para + "\n\n"
            else:
                chunks.append(current_chunk)
                current_chunk = para + "\n\n"
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks
```

**3. æ··åˆæœç´¢ç­–ç•¥**

```python
class HybridSearcher:
    """æ··åˆæœç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…"""
    
    def __init__(self, vectorstore, keyword_index):
        self.vectorstore = vectorstore
        self.keyword_index = keyword_index
    
    def search(self, query: str, top_k: int = 10) -> List[dict]:
        # å‘é‡æœç´¢
        vector_results = self.vectorstore.similarity_search_with_score(
            query, k=top_k * 2
        )
        
        # å…³é”®è¯æœç´¢
        keyword_results = self.keyword_index.search(query, k=top_k * 2)
        
        # RRF (Reciprocal Rank Fusion) èåˆ
        scores = {}
        
        # å‘é‡ç»“æœæ‰“åˆ†
        for rank, (doc, score) in enumerate(vector_results):
            doc_id = doc.metadata.get('id')
            scores[doc_id] = scores.get(doc_id, 0) + 1.0 / (rank + 60)
        
        # å…³é”®è¯ç»“æœæ‰“åˆ†
        for rank, doc in enumerate(keyword_results):
            doc_id = doc.metadata.get('id')
            scores[doc_id] = scores.get(doc_id, 0) + 1.0 / (rank + 60)
        
        # æ’åºå¹¶è¿”å›
        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_docs[:top_k]
```

**4. æ€§èƒ½ä¼˜åŒ–**

```yaml
ç´¢å¼•é€‰æ‹©æŒ‡å—:
  å°æ•°æ®é›† (< 100K):
    - Flat: ç²¾ç¡®æœç´¢ï¼Œ100% å¬å›
    
  ä¸­ç­‰æ•°æ®é›† (100K - 10M):
    - IVF: å¹³è¡¡èµ„æºå’Œæ€§èƒ½
    - HNSW: é«˜å¬å›ï¼Œå¿«é€ŸæŸ¥è¯¢
    
  å¤§æ•°æ®é›† (> 10M):
    - HNSW + PQ: å‹ç¼© + å¿«é€Ÿ
    - IVF + PQ: æ›´ä½å†…å­˜
    
  è¶…å¤§æ•°æ®é›† (> 100M):
    - DISKANN: ç£ç›˜ç´¢å¼•
    - åˆ†ç‰‡ + åˆ†å¸ƒå¼

å†…å­˜ä¼˜åŒ–:
  - é‡åŒ–: FP32 -> FP16 -> INT8
  - ç»´åº¦é™ç»´: PCAã€UMAP
  - åˆ†å±‚ç´¢å¼•: ç²—ç­› + ç²¾æ’
```

### å¸¸è§é™·é˜±

**1. ç»´åº¦è¯…å’’**
- âŒ ç›²ç›®å¢åŠ å‘é‡ç»´åº¦
- âœ… é€‰æ‹©é€‚åˆä»»åŠ¡çš„åµŒå…¥æ¨¡å‹
- âœ… é«˜ç»´æ•°æ®è€ƒè™‘é™ç»´

**2. è¯­ä¹‰æ¼‚ç§»**
- âŒ é•¿æœŸä¸æ›´æ–°åµŒå…¥æ¨¡å‹
- âœ… å®šæœŸè¯„ä¼°æ£€ç´¢è´¨é‡
- âœ… ç‰ˆæœ¬åŒ–ç®¡ç†åµŒå…¥æ¨¡å‹

**3. ä¸Šä¸‹æ–‡ä¸¢å¤±**
- âŒ åˆ†å—è¿‡å°å¯¼è‡´è¯­ä¹‰å‰²è£‚
- âœ… åˆç†çš„é‡å çª—å£
- âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†å—

**4. ç´¢å¼•é€‰æ‹©ä¸å½“**
- âŒ å¤§æ•°æ®é›†ä½¿ç”¨ Flat ç´¢å¼•
- âŒ å¿½è§†ç´¢å¼•å‚æ•°è°ƒä¼˜
- âœ… æ ¹æ®æ•°æ®è§„æ¨¡å’ŒæŸ¥è¯¢æ¨¡å¼é€‰æ‹©ç´¢å¼•

## ğŸ“š èµ„æºç´¢å¼•

### å­¦æœ¯è®ºæ–‡

1. **Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs** (2016)
   - ä½œè€…ï¼šYu. A. Malkov, D. A. Yashunin
   - é“¾æ¥ï¼šhttps://arxiv.org/abs/1603.09320
   - è¯´æ˜ï¼šHNSW ç®—æ³•åŸå§‹è®ºæ–‡

2. **Product Quantization for Nearest Neighbor Search** (2011)
   - ä½œè€…ï¼šHervÃ© JÃ©gou et al.
   - é“¾æ¥ï¼šhttps://hal.inria.fr/inria-00514462v2/document
   - è¯´æ˜ï¼šPQ é‡åŒ–æŠ€æœ¯

3. **FAISS: A library for efficient similarity search** (2017)
   - ä½œè€…ï¼šFacebook AI
   - é“¾æ¥ï¼šhttps://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/
   - è¯´æ˜ï¼šFAISS åº“ä»‹ç»

### æŠ€æœ¯æ–‡æ¡£

1. **Pinecone æ–‡æ¡£**
   - https://docs.pinecone.io/
   - æ‰˜ç®¡å‘é‡æ•°æ®åº“æŒ‡å—

2. **Milvus æ–‡æ¡£**
   - https://milvus.io/docs
   - å¼€æºå‘é‡æ•°æ®åº“

3. **Chroma æ–‡æ¡£**
   - https://docs.trychroma.com/
   - AI åŸç”ŸåµŒå…¥æ•°æ®åº“

4. **FAISS Wiki**
   - https://github.com/facebookresearch/faiss/wiki
   - Facebook ç›¸ä¼¼æ€§æœç´¢åº“

### å¼€æºé¡¹ç›®

1. **LangChain**
   - https://github.com/langchain-ai/langchain
   - å‘é‡å­˜å‚¨é›†æˆ

2. **LlamaIndex**
   - https://github.com/run-llama/llama_index
   - æ•°æ®ç´¢å¼•å’Œæ£€ç´¢

3. **Sentence Transformers**
   - https://github.com/UKPLab/sentence-transformers
   - æ–‡æœ¬åµŒå…¥æ¨¡å‹

4. **Vespa**
   - https://github.com/vespa-engine/vespa
   - å¤§è§„æ¨¡å‘é‡æœç´¢å¼•æ“

### è¯„ä¼°å·¥å…·

1. **ANN Benchmarks**
   - https://github.com/erikbern/ann-benchmarks
   - å‘é‡æ£€ç´¢ç®—æ³•åŸºå‡†æµ‹è¯•

2. **BEIR**
   - https://github.com/beir-cellar/beir
   - ä¿¡æ¯æ£€ç´¢è¯„ä¼°æ¡†æ¶

## ğŸ”— å…³è”çŸ¥è¯†

```mermaid
graph TD
    C03[C03_Vector_Databases]
    
    C03 --> C01[C01_Database_Sharding]
    C03 --> C02[C02_Data_Lakes_vs_Warehouses]
    
    C03 -.-> A05[A05_Spec_Expertise/B01_AI_LLM_Engineering]
    C03 -.-> A03[A03_Design_Architecture/B04_Middleware]
    C03 -.-> A01[A01_Infrastructure/B10_Cloud_Platforms]
    
    C03 --> |RAG ç³»ç»Ÿ| C01_Prompt[C01_Prompt_Engineering]
    C03 --> |æ¨èç³»ç»Ÿ| A08[A08_Domain_Applications]
```

## ğŸ’¡ å­¦ä¹ å»ºè®®

### å‰ç½®çŸ¥è¯†
- çº¿æ€§ä»£æ•°åŸºç¡€ï¼ˆå‘é‡ã€çŸ©é˜µï¼‰
- æœºå™¨å­¦ä¹ åµŒå…¥æ¦‚å¿µ
- æ•°æ®åº“ç´¢å¼•åŸç†
- Python æ•°æ®å¤„ç†

### å­¦ä¹ è·¯å¾„

**ç¬¬1å‘¨ï¼šåŸºç¡€æ¦‚å¿µ**
- ç†è§£å‘é‡åµŒå…¥
- å­¦ä¹ ç›¸ä¼¼åº¦åº¦é‡
- ä½“éªŒä¸åŒå‘é‡åº“

**ç¬¬2å‘¨ï¼šç´¢å¼•ç®—æ³•**
- HNSWã€IVF åŸç†
- ç´¢å¼•å‚æ•°è°ƒä¼˜
- æ€§èƒ½åŸºå‡†æµ‹è¯•

**ç¬¬3å‘¨ï¼šRAG å®è·µ**
- æ–‡æ¡£å¤„ç†ç®¡é“
- å‘é‡æ£€ç´¢ + LLM
- è¯„ä¼°å’Œä¼˜åŒ–

**ç¬¬4å‘¨ï¼šç”Ÿäº§éƒ¨ç½²**
- åˆ†å¸ƒå¼å‘é‡åº“
- ç›‘æ§å’Œè¿ç»´
- æˆæœ¬æ§åˆ¶

### å®è·µé¡¹ç›®

**é¡¹ç›®1ï¼šä¼ä¸šçŸ¥è¯†åº“é—®ç­”**
- æ–‡æ¡£å‘é‡åŒ–
- è¯­ä¹‰æ£€ç´¢
- é—®ç­”æœºå™¨äºº

**é¡¹ç›®2ï¼šå›¾ç‰‡æœç´¢å¼•æ“**
- å›¾åƒç‰¹å¾æå–
- ä»¥å›¾æœå›¾
- ç›¸ä¼¼å›¾ç‰‡æ¨è

**é¡¹ç›®3ï¼šä»£ç è¯­ä¹‰æœç´¢**
- ä»£ç åµŒå…¥
- è‡ªç„¶è¯­è¨€æœä»£ç 
- ç›¸ä¼¼ä»£ç æ£€æµ‹

## ğŸ”„ ç»´æŠ¤è¯´æ˜

- **æ›´æ–°é¢‘ç‡**: æ¯å­£åº¦æ›´æ–°å‘é‡åº“æ–°ç‰ˆæœ¬ç‰¹æ€§
- **è´¨é‡æ ‡å‡†**: ä»£ç ç¤ºä¾‹å¯è¿è¡Œï¼Œæ¨¡å‹é“¾æ¥æœ‰æ•ˆ
- **è´¡çŒ®æ–¹å¼**: æäº¤æ–°çš„æ£€ç´¢æŠ€å·§ã€ä¼˜åŒ–æ–¹æ³•ã€å®è·µæ¡ˆä¾‹
